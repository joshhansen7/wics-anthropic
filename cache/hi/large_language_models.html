# बड़े भाषा मॉडल

बड़े भाषा मॉडल (अंग्रेज़ी: Large Language Model, संक्षेप में LLM) एक प्रकार का भाषा मॉडल है जिसे स्व-पर्यवेक्षित मशीन लर्निंग का उपयोग करके विशाल मात्रा में पाठ पर प्रशिक्षित किया जाता है, और जो विशेष रूप से प्राकृतिक भाषा प्रसंस्करण कार्यों, विशेषकर भाषा उत्पादन के लिए डिज़ाइन किया गया है। सबसे बड़े और सबसे क्षमतावान LLM जेनरेटिव प्री-ट्रेंड ट्रांसफॉर्मर (GPTs) हैं और ChatGPT, Gemini और Claude जैसे चैटबॉट की मूल क्षमताएँ प्रदान करते हैं। LLM को विशिष्ट कार्यों के लिए फाइन-ट्यून किया जा सकता है या प्रॉम्प्ट इंजीनियरिंग द्वारा निर्देशित किया जा सकता है। ये मॉडल मानव भाषा कॉर्पोरा में निहित वाक्यविन्यास, अर्थशास्त्र और ऑन्टोलॉजी के संबंध में भविष्यवाणी शक्ति प्राप्त करते हैं, लेकिन वे प्रशिक्षण डेटा में मौजूद अशुद्धियों और पूर्वाग्रहों को भी विरासत में ले लेते हैं।

LLM में अरबों से खरबों तक के पैरामीटर होते हैं और ये सामान्य-उद्देश्य वाले अनुक्रम मॉडल के रूप में कार्य करते हैं, जो पाठ उत्पन्न करने, सारांशित करने, अनुवाद करने और तर्क करने में सक्षम होते हैं। LLM अपनी क्षमता में एक महत्वपूर्ण नई प्रौद्योगिकी का प्रतिनिधित्व करते हैं, जो न्यूनतम कार्य-विशिष्ट पर्यवेक्षण के साथ कार्यों को सामान्यीकृत करने की क्षमता रखते हैं, जिससे संवादात्मक एजेंट, कोड जनरेशन, ज्ञान पुनर्प्राप्ति और स्वचालित तर्क जैसी क्षमताएं सक्षम होती हैं जिनके लिए पहले विशेष रूप से निर्मित प्रणालियों की आवश्यकता होती थी।

## इतिहास

2017 के [ट्रांसफॉर्मर आर्किटेक्चर](/article/hi/ट्रांसफॉर्मर_आर्किटेक्चर) से पहले, कुछ भाषा मॉडल को अपने समय की कम्प्यूटेशनल और डेटा सीमाओं के सापेक्ष बड़ा माना जाता था। 1990 के दशक की शुरुआत में, IBM के सांख्यिकीय मॉडलों ने मशीन अनुवाद के लिए शब्द संरेखण तकनीकों का मार्ग प्रशस्त किया, जिससे कॉर्पस-आधारित भाषा मॉडलिंग की नींव रखी गई। 2001 में, 300 मिलियन शब्दों पर प्रशिक्षित स्मूथ n-ग्राम मॉडल, जैसे कि वे जो [Kneser–Ney स्मूथिंग](/article/hi/Kneser–Ney_स्मूथिंग) का उपयोग करते हैं, ने बेंचमार्क परीक्षणों पर अपने समय के सर्वश्रेष्ठ परप्लेक्सिटी हासिल की। 2000 के दशक में, व्यापक इंटरनेट पहुंच के प्रसार के साथ, शोधकर्ताओं ने सांख्यिकीय भाषा मॉडल को प्रशिक्षित करने के लिए वेब से विशाल पाठ डेटासेट ("वेब एज़ कॉर्पस") संकलित करना शुरू किया।

### न्यूरल नेटवर्क का उदय

2000 की शुरुआत में, शोधकर्ताओं ने भाषा मॉडल सीखने के लिए न्यूरल नेटवर्क का उपयोग करना शुरू किया। 2012 के आसपास छवि वर्गीकरण में गहरे न्यूरल नेटवर्क की सफलता के बाद, समान आर्किटेक्चर को भाषा कार्यों के लिए अनुकूलित किया गया। इस बदलाव को [Word2Vec](/article/hi/Word2Vec) जैसे शब्द एम्बेडिंग (2013 में [Mikolov](/article/hi/Tomas_Mikolov) द्वारा) और [LSTM](/article/hi/LSTM) का उपयोग करके सीक्वेंस-टू-सीक्वेंस (seq2seq) मॉडल के विकास से चिह्नित किया गया। 2016 में, Google ने अपनी अनुवाद सेवा को सांख्यिकीय वाक्यांश-आधारित मॉडलों के स्थान पर गहरे [पुनरावर्ती न्यूरल नेटवर्क](/article/hi/पुनरावर्ती_न्यूरल_नेटवर्क) का उपयोग करके [न्यूरल मशीन अनुवाद](/article/hi/न्यूरल_मशीन_अनुवाद) (NMT) में स्थानांतरित कर दिया। ये प्रारंभिक NMT सिस्टम LSTM-आधारित एनकोडर-डिकोडर आर्किटेक्चर का उपयोग करते थे, क्योंकि वे ट्रांसफॉर्मर के आविष्कार से पहले के थे।

### ट्रांसफॉर्मर क्रांति

2017 के [NeurIPS](/article/hi/NeurIPS) सम्मेलन में, Google के शोधकर्ताओं ने अपने महत्वपूर्ण पेपर "[अटेंशन इज़ ऑल यू नीड](/article/hi/अटेंशन_इज़_ऑल_यू_नीड)" में ट्रांसफॉर्मर आर्किटेक्चर पेश किया। इस पेपर का उद्देश्य 2014 की seq2seq तकनीक को बेहतर बनाना था और यह मुख्य रूप से 2014 में [बहदानऊ और अन्य](/article/hi/बहदानऊ) द्वारा विकसित अटेंशन मैकेनिज्म पर आधारित था। अगले वर्ष 2018 में, [BERT](/article/hi/BERT) पेश किया गया और जल्दी ही "सर्वव्यापी" हो गया। हालांकि मूल ट्रांसफॉर्मर में एनकोडर और डिकोडर दोनों ब्लॉक थे, BERT एक केवल-एनकोडर मॉडल है। शैक्षिक और अनुसंधान उपयोग में BERT 2023 में गिरना शुरू हो गया, क्योंकि केवल-डिकोडर मॉडल (जैसे [GPT](/article/hi/GPT)) की कार्यों को प्रॉम्प्टिंग के माध्यम से हल करने की क्षमताओं में तेजी से सुधार हुआ।

### GPT मॉडल का विकास

हालांकि केवल-डिकोडर [GPT-1](/article/hi/GPT-1) 2018 में पेश किया गया था, 2019 में [GPT-2](/article/hi/GPT-2) ने व्यापक ध्यान आकर्षित किया क्योंकि OpenAI ने शुरू में इसे दुरुपयोग के डर से सार्वजनिक रूप से जारी करने के लिए बहुत शक्तिशाली माना था। 2020 में [GPT-3](/article/hi/GPT-3) ने एक और कदम आगे बढ़ाया, और 2025 तक यह केवल API के माध्यम से उपलब्ध है, मॉडल को स्थानीय रूप से चलाने के लिए डाउनलोड करने की कोई पेशकश नहीं है। लेकिन 2022 के उपभोक्ता-केंद्रित चैटबॉट [ChatGPT](/article/hi/ChatGPT) ने व्यापक मीडिया कवरेज और जन ध्यान प्राप्त किया। 2023 के [GPT-4](/article/hi/GPT-4) को इसकी बढ़ी हुई सटीकता और बहुसंवेदी क्षमताओं के लिए "होली ग्रेल" के रूप में सराहा गया। OpenAI ने GPT-4 के उच्च-स्तरीय आर्किटेक्चर और पैरामीटर संख्या का खुलासा नहीं किया। ChatGPT के लॉन्च से कंप्यूटर विज्ञान के कई अनुसंधान उप-क्षेत्रों में LLM के उपयोग में वृद्धि हुई, जिसमें रोबोटिक्स, सॉफ्टवेयर इंजीनियरिंग और सामाजिक प्रभाव कार्य शामिल हैं। 2024 में OpenAI ने [तर्क मॉडल OpenAI o1](/article/hi/OpenAI_o1) लॉन्च किया, जो अंतिम उत्तर देने से पहले लंबी विचार श्रृंखलाएं उत्पन्न करता है। OpenAI के GPT श्रृंखला के समान पैरामीटर गिनती वाले कई LLM विकसित किए गए हैं।

### ओपन सोर्स मॉडल का उदय

2022 से, ओपन-सोर्स मॉडल लोकप्रिय हो गए हैं, विशेष रूप से शुरुआत में [BLOOM](/article/hi/BLOOM) और [LLaMA](/article/hi/LLaMA), हालांकि दोनों के उपयोग क्षेत्र पर प्रतिबंध हैं। [Mistral AI](/article/hi/Mistral_AI) के मॉडल [Mistral 7B](/article/hi/Mistral_7B) और [Mixtral 8x7b](/article/hi/Mixtral_8x7b) अधिक उदार अपाचे लाइसेंस के साथ आते हैं। जनवरी 2025 में, [DeepSeek](/article/hi/DeepSeek) ने [DeepSeek R1](/article/hi/DeepSeek_R1) जारी किया, एक 671 अरब पैरामीटर वाला ओपन-वेट मॉडल जो OpenAI o1 के तुलनीय प्रदर्शन करता है लेकिन बहुत कम लागत पर।

### मल्टीमॉडल प्रगति

2023 से, कई LLM को बहुसंवेदी होने के लिए प्रशिक्षित किया गया है, जिससे उन्हें छवियों या ऑडियो जैसे अन्य प्रकार के डेटा को संसाधित या उत्पन्न करने की क्षमता मिली है। इन LLM को बड़े बहुसंवेदी मॉडल ([LMMs](/article/hi/लार्ज_मल्टीमॉडल_मॉडल)) भी कहा जाता है।

2024 तक, सबसे बड़े और सबसे क्षमतावान मॉडल ट्रांसफॉर्मर आर्किटेक्चर पर आधारित हैं। कुछ हाल के कार्यान्वयन अन्य आर्किटेक्चर पर आधारित हैं, जैसे पुनरावर्ती न्यूरल नेटवर्क वेरिएंट और [मांबा](/article/hi/मांबा) (एक स्टेट स्पेस मॉडल)।

### ओपन-सोर्स का महत्व

ओपन-सोर्स LLM ने 2023 से क्षेत्र को महत्वपूर्ण रूप से आकार दिया है, जिससे AI विकास में व्यापक भागीदारी और मॉडल मूल्यांकन में अधिक पारदर्शिता को बढ़ावा मिला है। वाके और अन्य (2025) ने प्रदर्शित किया कि ओपन-सोर्स मॉडलों में समुदाय-संचालित योगदान मापने योग्य रूप से उनकी दक्षता और प्रदर्शन में सुधार करते हैं, जहां [Hugging Face](/article/hi/Hugging_Face) जैसे सहयोगी प्लेटफार्मों पर उपयोगकर्ता भागीदारी तेजी से बढ़ रही है। पेरिस और अन्य (2025) ने तर्क दिया कि AI में खुलापन को केवल मॉडल कोड या वेट्स जारी करने से परे, AI अनुसंधान और तैनाती में समावेशिता, जवाबदेही और नैतिक जिम्मेदारी तक विस्तारित करना चाहिए। सामूहिक रूप से, ये अध्ययन इस बात पर प्रकाश डालते हैं कि ओपन-सोर्स LLM नवाचार को तेज कर सकते हैं और वैज्ञानिक पुनरुत्पादनीयता को बढ़ा सकते हैं, जबकि अधिक पारदर्शी और भागीदारी वाले AI पारिस्थितिकी तंत्र को बढ़ावा देते हैं।

## डेटासेट प्रीप्रोसेसिंग

### टोकनाइजेशन

चूंकि मशीन लर्निंग एल्गोरिदम संख्याओं को संसाधित करते हैं न कि पाठ को, इसलिए पाठ को संख्याओं में परिवर्तित करना आवश्यक है। पहले चरण में, एक शब्दावली तय की जाती है, फिर प्रत्येक शब्दावली प्रविष्टि को मनमाने ढंग से लेकिन अद्वितीय पूर्णांक इंडेक्स असाइन किया जाता है, और अंत में, पूर्णांक इंडेक्स के साथ एक एम्बेडिंग जुड़ा होता है। इसके लिए उपयोग किए जाने वाले एल्गोरिदम में [बाइट-पेयर एनकोडिंग](/article/hi/बाइट-पेयर_एनकोडिंग) (BPE) और [WordPiece](/article/hi/WordPiece) शामिल हैं। कुछ विशेष टोकन नियंत्रण वर्णों के रूप में कार्य करते हैं, जैसे [MASK] मास्क किए गए टोकन के लिए (जैसा BERT में उपयोग किया जाता है), और [UNK] ("अज्ञात") शब्दावली में नहीं आने वाले वर्णों के लिए। इसके अतिरिक्त, विशेष पाठ फॉर्मेटिंग को दर्शाने के लिए कुछ विशेष प्रतीक भी उपयोग किए जाते हैं। उदाहरण के लिए, "Ġ" RoBERTa और GPT में पिछले व्हाइटस्पेस को दर्शाता है। "##" BERT में पिछले शब्द के जारी रहने को दर्शाता है।

उदाहरण के लिए, GPT-3 (लेगेसी) द्वारा उपयोग किया जाने वाला BPE टोकनाइज़र टोकनाइज़र: पाठ -> संख्यात्मक "टोकन" की श्रृंखला में विभाजित करेगा।

टोकनाइज़ेशन डेटासेट को भी संपीड़ित करता है। चूंकि LLM आमतौर पर एक सरणी के इनपुट की आवश्यकता होती है जो जैग्ड न हो, छोटे पाठों को सबसे लंबे वाले के साथ मेल खाने तक "पैड" किया जाना चाहिए। प्रति शब्द औसत टोकन की संख्या भाषा पर निर्भर करती है। अंग्रेजी में, अनुपात आमतौर पर प्रति टोकन 0.75 शब्द के आसपास होता है, जिसमें औसतन प्रति टोकन 4 वर्ण होते हैं।

#### बाइट-पेयर एनकोडिंग

उदाहरण के लिए, बाइट-पेयर एनकोडिंग पर आधारित एक टोकनाइज़र पर विचार करें। पहले चरण में, सभी अद्वितीय वर्ण (रिक्त स्थान और विराम चिह्न सहित) को n-ग्राम के प्रारंभिक सेट के रूप में माना जाता है (यानी, यूनिग्राम का प्रारंभिक सेट)। क्रमिक रूप से, आसन्न वर्णों के सबसे अधिक होने वाले जोड़े को एक बाईग्राम में मर्ज किया जाता है और जोड़े के सभी उदाहरणों को इससे प्रतिस्थापित किया जाता है। फिर, (पहले से मर्ज किए गए) n-ग्राम के आसन्न जोड़ों के सभी होने वाले जोड़ों को फिर से लंबे n-ग्राम में मर्ज किया जाता है, जब तक कि निर्धारित आकार का शब्दकोश प्राप्त न हो जाए। टोकनाइज़र के प्रशिक्षित होने के बाद, कोई भी पाठ इससे टोकनाइज़ किया जा सकता है, बशर्ते कि इसमें प्रारंभिक यूनिग्राम सेट में नहीं आने वाले कोई वर्ण न हों।

#### समस्याएँ

मुख्य रूप से अंग्रेजी कॉर्पस से निकाली गई आवृत्तियों पर आधारित टोकन शब्दावली एक औसत अंग्रेजी शब्द के लिए यथासंभव कम टोकन का उपयोग करती है। हालांकि, ऐसे अंग्रेजी-अनुकूलित टोकनाइज़र द्वारा एनकोडेड किसी अन्य भाषा के औसत शब्द को अनुकूल मात्रा के टोकन में विभाजित किया जाता है। उदाहरण के लिए, म्यांमार की शान भाषा के लिए GPT-2 टोकनाइज़र प्रति शब्द 15 गुना तक अधिक टोकन का उपयोग कर सकता है। पुर्तगाली और जर्मन जैसी अधिक व्यापक भाषाओं में भी अंग्रेजी की तुलना में "50% प्रीमियम" होता है।

### डेटासेट क्लीनिंग

LLM के प्रशिक्षण के संदर्भ में, डेटासेट को आमतौर पर निम्न-गुणवत्ता, डुप्लिकेट, या हानिकारक डेटा को हटाकर साफ किया जाता है। साफ किए गए डेटासेट प्रशिक्षण दक्षता बढ़ा सकते हैं और डाउनस्ट्रीम प्रदर्शन में सुधार की ओर ले जा सकते हैं। एक प्रशिक्षित LLM का उपयोग आगे के LLM के प्रशिक्षण के लिए डेटासेट को साफ करने के लिए किया जा सकता है।

वेब पर LLM-जनित सामग्री के बढ़ते अनुपात के साथ, भविष्य में डेटा क्लीनिंग में ऐसी सामग्री को फ़िल्टर करना शामिल हो सकता है। LLM-जनित सामग्री एक समस्या हो सकती है यदि सामग्री मानव पाठ के समान है (जिससे फ़िल्टरिंग कठिन हो जाती है) लेकिन गुणवत्ता में कमी है (जिससे इस पर प्रशिक्षित मॉडल का प्रदर्शन कम हो जाता है)।

### सिंथेटिक डेटा

सबसे बड़े भाषा मॉडलों के प्रशिक्षण के लिए प्राकृतिक रूप से उपलब्ध से अधिक भाषाई डेटा की आवश्यकता हो सकती है, या प्राकृतिक रूप से होने वाला डेटा अपर्याप्त गुणवत्ता का हो सकता है। इन मामलों में, सिंथेटिक डेटा का उपयोग किया जा सकता है। Microsoft की Phi श्रृंखला के LLM को एक अन्य LLM द्वारा उत्पन्न पाठ्यपुस्तक जैसे डेटा पर प्रशिक्षित किया गया है।

## प्रशिक्षण

LLM एक प्रकार का [फाउंडेशन मॉडल](/article/hi/फाउंडेशन_मॉडल) (बड़ा X मॉडल) है जो भाषा पर प्रशिक्षित है। LLM को विभिन्न तरीकों से प्रशिक्षित किया जा सकता है। विशेष रूप से, GPT मॉडल को पहले बड़ी मात्रा में डेटा पर अगले शब्द की भविष्यवाणी करने के लिए प्री-ट्रेन किया जाता है, इसके बाद फाइन-ट्यूनिंग की जाती है।

### लागत

सबसे बड़े मॉडलों को प्रशिक्षित करने के लिए पर्याप्त इंफ्रास्ट्रक्चर आवश्यक है। बड़े मॉडलों की ओर प्रवृत्ति बड़े भाषा मॉडलों की सूची में देखी जा सकती है। उदाहरण के लिए, 2019 में GPT-2 (यानी 1.5 अरब पैरामीटर मॉडल) के प्रशिक्षण की लागत $50,000 थी, जबकि 2022 में PaLM (यानी 540 अरब पैरामीटर मॉडल) के प्रशिक्षण की लागत $8 मिलियन थी, और Megatron-Turing NLG 530B (2021) की लागत लगभग $11 मिलियन थी। "बड़ा भाषा मॉडल" में विशेषण "बड़ा" अपने आप में अस्पष्ट है, क्योंकि "बड़े" को परिभाषित करने के लिए पैरामीटर संख्या की कोई निश्चित सीमा नहीं है। 2018 का GPT-1 में 117 मिलियन पैरामीटर थे।

### फाइन-ट्यूनिंग

प्री-ट्रेन होने से पहले, अधिकांश LLM अगले-टोकन प्रेडिक्टर हैं। फाइन-ट्यूनिंग LLM के आउटपुट को [रीइन्फोर्समेंट लर्निंग फ्रॉम ह्यूमन फीडबैक](/article/hi/रीइन्फोर्समेंट_लर्निंग_फ्रॉम_ह्यूमन_फीडबैक) (RLHF) या [कॉन्स्टिट्यूशनल AI](/article/hi/कॉन्स्टिट्यूशनल_AI) जैसी तकनीकों के माध्यम से अधिक वार्तालाप की तरह दिखने के लिए समायोजित करती है।

[इंस्ट्रक्शन फाइन-ट्यूनिंग](/article/hi/इंस्ट्रक्शन_फाइन-ट्यूनिंग) पर्यवेक्षित लर्निंग का एक रूप है जिसका उपयोग LLM को उपयोगकर्ता के निर्देशों का पालन करने के लिए सिखाने के लिए किया जाता है। 2022 में, OpenAI ने [InstructGPT](/article/hi/InstructGPT) का प्रदर्शन किया, GPT-3 का एक संस्करण जिसे इसी तरह से निर्देशों का पालन करने के लिए फाइन-ट्यून किया गया था।

[रीइन्फोर्समेंट लर्निंग फ्रॉम ह्यूमन फीडबैक](/article/hi/रीइन्फोर्समेंट_लर्निंग_फ्रॉम_ह्यूमन_फीडबैक) (RLHF) में एक रिवॉर्ड मॉडल को प्रशिक्षित करना शामिल है जो यह भविष्यवाणी करता है कि मनुष्य किस पाठ को पसंद करते हैं। फिर, LLM को इस रिवॉर्ड मॉडल को संतुष्ट करने के लिए रीइन्फोर्समेंट लर्निंग के माध्यम से फाइन-ट्यून किया जा सकता है। चूंकि मनुष्य आमतौर पर सत्य, सहायक और हानिरहित उत्तरों को पसंद करते हैं, RLHF ऐसे उत्तरों को प्राथमिकता देता है।

## आर्किटेक्चर

LLM आम तौर पर [ट्रांसफॉर्मर आर्किटेक्चर](/article/hi/ट्रांसफॉर्मर_आर्किटेक्चर) पर आधारित होते हैं, जो एक ध्यान तंत्र का उपयोग करता है जो मॉडल को अनुक्रम के सभी तत्वों के बीच संबंधों को एक साथ संसाधित करने की अनुमति देता है, चाहे वे एक-दूसरे से कितनी भी दूर हों।

### अटेंशन मैकेनिज्म और कॉन्टेक्स्ट विंडो

कॉन्टेक्स्ट विंडो के भीतर कौन से टोकन एक-दूसरे के लिए प्रासंगिक हैं, यह पता लगाने के लिए, अटेंशन मैकेनिज़्म प्रत्येक टोकन के लिए, अधिक सटीक रूप से इसकी एम्बेडिंग के लिए, "सॉफ्ट" वेट्स की गणना करता है, कई अटेंशन हेड का उपयोग करके, प्रत्येक के पास अपनी "प्रासंगिकता" है जिसका उपयोग अपने स्वयं के सॉफ्ट वेट्स की गणना के लिए किया जाता है। उदाहरण के लिए, छोटा (यानी 117M पैरामीटर आकार) GPT-2 मॉडल में बारह अटेंशन हेड और केवल 1k टोकन का कॉन्टेक्स्ट विंडो है। इसके मध्यम संस्करण में 345M पैरामीटर हैं और 24 परतें हैं, प्रत्येक परत में 12 अटेंशन हेड हैं। प्रशिक्षण के लिए ग्रेडिएंट डिसेंट के साथ, 512 का बैच आकार उपयोग किया गया था।

Google के [Gemini 1.5](/article/hi/Gemini_1.5), फरवरी 2024 में पेश किया गया, 1 मिलियन टोकन तक का कॉन्टेक्स्ट विंडो रख सकता है।

मॉडल को या तो अनुक्रम के जारी रहने की भविष्यवाणी करने के लिए प्री-ट्रेन किया जा सकता है, या अनुक्रम में लापता भागों को भरने के लिए, जो प्रशिक्षण डेटासेट से एक अनुक्रम दिए गए पर आधारित होता है। यह या तो हो सकता है:

- ऑटोरेग्रेसिव (यानी, अनुक्रम के जारी रहने की भविष्यवाणी, जैसे GPT करते हैं): उदाहरण के लिए, "मुझे खाना पसंद है" जैसा अनुक्रम दिए जाने पर, मॉडल "आइसक्रीम" या "सुशी" की भविष्यवाणी करता है।
- "मास्क्ड" (यानी, अनुक्रम से अनुपस्थित भागों को भरना, जैसा "BERT" करता है): उदाहरण के लिए, "मैं [__] [__] क्रीम" जैसा अनुक्रम दिए जाने पर, मॉडल भविष्यवाणी करता है कि "खाना" और "आइस" अनुपस्थित हैं।

मॉडल को सहायक कार्यों पर भी प्रशिक्षित किया जा सकता है जो डेटा वितरण की उनकी समझ का परीक्षण करते हैं, जैसे अगले वाक्य की भविष्यवाणी (NSP), जिसमें वाक्यों के जोड़े प्रस्तुत किए जाते हैं और मॉडल को यह भविष्यवाणी करनी होती है कि क्या वे प्रशिक्षण कॉर्पस में क्रमिक रूप से दिखाई देते हैं। प्रशिक्षण के दौरान, प्रशिक्षण को स्थिर रखने के लिए रेगुलराइज़ेशन लॉस का भी उपयोग किया जाता है। हालांकि, रेगुलराइज़ेशन लॉस आमतौर पर परीक्षण और मूल्यांकन के दौरान उपयोग नहीं की जाती है।

### मिक्स्चर ऑफ एक्सपर्ट्स

[मिक्स्चर ऑफ एक्सपर्ट्स](/article/hi/मिक्स्चर_ऑफ_एक्सपर्ट्स) (MoE) एक मशीन लर्निंग आर्किटेक्चर है जिसमें कई विशेष न्यूरल नेटवर्क ("एक्सपर्ट्स") एक साथ काम करते हैं, गेटिंग मैकेनिज़्म के साथ जो प्रत्येक इनपुट को सबसे उपयुक्त एक्सपर्ट(स) में रूट करता है। मिक्स्चर ऑफ एक्सपर्ट्स इन्फरेंस लागत को कम कर सकते हैं, क्योंकि प्रत्येक इनपुट के लिए पैरामीटर्स का केवल एक अंश ही उपयोग किया जाता है। इस दृष्टिकोण को 2017 में Google के शोधकर्ताओं द्वारा पेश किया गया था।

### पैरामीटर आकार

आम तौर पर, LLM को सिंगल या हाफ-प्रेसिजन फ्लोटिंग पॉइंट नंबर्स (float32 और float16) के साथ प्रशिक्षित किया जाता है। एक float16 में 16 बिट, या 2 बाइट होते हैं, और इसलिए एक अरब पैरामीटर के लिए 2 गीगाबाइट की आवश्यकता होती है। सबसे बड़े मॉडलों में आमतौर पर 100 अरब पैरामीटर होते हैं, जिन्हें लोड करने के लिए 200 गीगाबाइट की आवश्यकता होती है, जो अधिकांश उपभोक्ता इलेक्ट्रॉनिक्स की श्रेणी से बाहर है।

### क्वांटिज़ेशन

पोस्ट-ट्रेनिंग क्वांटिज़ेशन का उद्देश्य प्रशिक्षित मॉडल के पैरामीटर की सटीकता को कम करके स्पेस आवश्यकता को कम करना है, जबकि इसके प्रदर्शन के अधिकांश भाग को संरक्षित रखा जाता है। क्वांटिज़ेशन को स्टैटिक क्वांटिज़ेशन के रूप में और वर्गीकृत किया जा सकता है यदि क्वांटिज़ेशन पैरामीटर पहले से निर्धारित हैं (आमतौर पर कैलिब्रेशन चरण के दौरान), और डायनामिक क्वांटिज़ेशन यदि क्वांटिज़ेशन इन्फरेंस के दौरान लागू किया जाता है। क्वांटिज़ेशन का सबसे सरल रूप सभी पैरामीटर को दिए गए बिट्स की संख्या तक काट देता है: यह स्टैटिक के साथ-साथ डायनामिक क्वांटिज़ेशन पर भी लागू है, लेकिन इसमें बहुत सटीकता खो जाती है। डायनामिक क्वांटिज़ेशन प्रति परत एक अलग क्वांटिज़ेशन कोडबुक का उपयोग करने की अनुमति देता है, या तो मूल्यों की एक लुकअप टेबल या एक रैखिक मैपिंग (स्केलिंग फैक्टर और बायस), जो कम-सटीकता अंकगणित का उपयोग करने से संभावित गति सुधार का त्याग करता है।

क्वांटिज़्ड मॉडल आमतौर पर फ्रोज़न माने जाते हैं, जिनके वेट्स में संशोधन (जैसे फाइन-ट्यूनिंग) केवल मूल मॉडल पर लागू होता है। [लो-रैंक एडैप्टेशन](/article/hi/लो-रैंक_एडैप्टेशन) का उपयोग करके क्वांटिज़्ड मॉडल को फाइन-ट्यून करना संभव है।

## विस्तार करने की क्षमता

बुनियादी पाठ जनरेशन के अलावा, LLM क्षमताओं को बढ़ाने के लिए विभिन्न तकनीकों का विकास किया गया है, जिनमें बाहरी उपकरणों और डेटा स्रोतों का उपयोग, जटिल समस्याओं पर बेहतर तर्क, और प्रॉम्प्टिंग विधियों के माध्यम से बेहतर निर्देश का पालन या स्वायत्तता शामिल हैं।

### प्रॉम्प्ट इंजीनियरिंग

2020 में, OpenAI के शोधकर्ताओं ने प्रदर्शित किया कि उनका नया मॉडल GPT-3 इनपुट डेटा में उदाहरण के रूप में प्रश्न और उत्तर (या अन्य प्रकार के कार्य) के कुछ राउंड दिए जाने पर किस प्रारूप का उपयोग करना है, यह समझ सकता था, आंशिक रूप से RLHF तकनीक के कारण। इस तकनीक, जिसे फ्यू-शॉट प्रॉम्प्टिंग कहा जाता है, LLM को फाइन-ट्यूनिंग की आवश्यकता के बिना किसी भी कार्य के लिए अनुकूलित करने की अनुमति देती है। 2022 में, यह पाया गया कि बेस GPT-3 मॉडल उपयोगकर्ता इनपुट के आधार पर एक निर्देश उत्पन्न कर सकता है। उत्पन्न निर्देश के साथ उपयोगकर्ता इनपुट को फिर दूसरे इंस्टेंस के इनपुट के रूप में "निर्देश: [...], इनपुट: [...], आउटपुट:" प्रारूप के तहत उपयोग किया जाता है। दूसरा इंस्टेंस आउटपुट को पूरा कर सकता है और अक्सर ऐसा करते हुए सही उत्तर प्रदान करता है। "सेल्फ-इंस्ट्रक्ट" करने की क्षमता LLM को सही उत्तर की ओर स्वयं बूटस्ट्रैप करने में सक्षम बनाती है।

### डायलॉग प्रोसेसिंग (चैटबॉट)

एक LLM को एक चैटबॉट या "डायलॉग असिस्टेंट" में बदला जा सकता है, इसे वार्तालाप के लिए विशेष बनाकर। मूल रूप से, उपयोगकर्ता इनपुट को एक मार्कर जैसे "Q:" या "उपयोगकर्ता:" के साथ प्रीफिक्स किया जाता है और LLM से एक निश्चित "A:" या "असिस्टेंट:" के बाद आउटपुट का अनुमान लगाने के लिए कहा जाता है। इस प्रकार के मॉडल को 2022 में ChatGPT के साथ वाणिज्यिक रूप से उपलब्ध कराया गया, जो GPT-3.5 पर आधारित InstructGPT का एक सिबलिंग मॉडल है जिसे डायलॉग-फॉर्मैटेड टेक्स्ट को स्वीकार करने और उत्पन्न करने के लिए फाइन-ट्यून किया गया है। यह इसी तरह उपयोगकर्ता के निर्देशों का पालन कर सकता था। उपयोगकर्ता और असिस्टेंट की लाइनों की स्ट्रीम से पहले, एक चैट संदर्भ आमतौर पर एक भूमिका जिसे "डेवलपर" या "सिस्टम" कहा जाता है, से कुछ पंक्तियों के ओवरआर्किंग निर्देशों के साथ शुरू होता है, जो उपयोगकर्ता के इनपुट से अधिक अधिकार व्यक्त करने के लिए। इसे "सिस्टम प्रॉम्प्ट" कहा जाता है।

### रिट्रीवल-ऑग्मेंटेड जनरेशन

[रिट्रीवल-ऑग्मेंटेड जनरेशन](/article/hi/रिट्रीवल-ऑग्मेंटेड_जनरेशन) (RAG) एक दृष्टिकोण है जो LLM को दस्तावेज़ पुनर्प्राप्ति प्रणालियों के साथ एकीकृत करके बढ़ाता है। एक क्वेरी दिए जाने पर, सबसे प्रासंगिक दस्तावेज़ों को पुनर्प्राप्त करने के लिए एक दस्तावेज़ पुनर्प्राप्तिकर्ता को बुलाया जाता है। यह आमतौर पर क्वेरी और दस्तावेज़ों को वेक्टर्स में एनकोड करके, फिर क्वेरी के वेक्टर के समान वेक्टर (आमतौर पर एक वेक्टर डेटाबेस में संग्रहीत) वाले दस्तावेज़ों को खोजकर किया जाता है। फिर LLM क्वेरी और पुनर्प्राप्त दस्तावेज़ों से शामिल संदर्भ दोनों के आधार पर एक आउटपुट उत्पन्न करता है।

### टूल उपयोग

टूल उपयोग एक तंत्र है जो LLM को बाहरी सिस्टम, अनुप्रयोगों या डेटा स्रोतों के साथ बातचीत करने की अनुमति देता है। यह उदाहरण के लिए किसी API से रीयल-टाइम जानकारी प्राप्त करने या कोड निष्पादित करने की अनुमति दे सकता है। LLM से अलग एक प्रोग्राम LLM के आउटपुट स्ट्रीम पर विशेष टूल-कॉलिंग सिंटैक्स के लिए नजर रखता है। जब ये विशेष टोकन दिखाई देते हैं, तो प्रोग्राम तदनुसार टूल को कॉल करता है और इसके आउटपुट को LLM की इनपुट स्ट्रीम में वापस फीड करता है।

प्रारंभिक टूल-उपयोग LLM विशिष्ट उपकरणों के उपयोग के लिए फाइन-ट्यून किए गए थे। लेकिन LLM को API दस्तावेज़ पढ़ने और API को सही ढंग से कॉल करने की क्षमता के लिए फाइन-ट्यून करना LLM के लिए सुलभ उपकरणों की सीमा को काफी विस्तारित कर दिया है। सिस्टम प्रॉम्प्ट में उपलब्ध टूल का वर्णन करना भी LLM को टूल का उपयोग करने में सक्षम बना सकता है। ChatGPT (GPT-4) को कई प्रकार के टूल का उपयोग करने के निर्देश देने वाला एक सिस्टम प्रॉम्प्ट ऑनलाइन पाया जा सकता है।

### एजेंसी

एक LLM अपने आप में एक स्वायत्त एजेंट नहीं है, क्योंकि इसमें गतिशील वातावरण के साथ बातचीत करने, पिछले व्यवहारों को याद रखने और भविष्य की कार्रवाइयों की योजना बनाने की क्षमता की कमी है। लेकिन इसे सहायक तत्वों को जोड़कर एक एजेंट में परिवर्तित किया जा सकता है: एजेंट की भूमिका (प्रोफाइल) और एजेंट का आसपास का वातावरण LLM के लिए अतिरिक्त इनपुट हो सकते हैं, जबकि मेमोरी को एक टूल के रूप में या अतिरिक्त इनपुट के रूप में एकीकृत किया जा सकता है। निर्देशों और इनपुट पैटर्न का उपयोग LLM को कार्रवाइयों की योजना बनाने के लिए किया जाता है और टूल उपयोग का उपयोग संभावित रूप से इन कार्रवाइयों को करने के लिए किया जाता है।

[ReAct](/article/hi/ReAct) पैटर्न, जो reason और act का संयोजन है, LLM का उपयोग एक प्लानर के रूप में करके LLM से एक एजेंट बनाता है। LLM को "जोर से सोचने" के लिए कहा जाता है। विशेष रूप से, भाषा मॉडल को वातावरण का पाठ्य विवरण, एक लक्ष्य, संभावित क्रियाओं की एक सूची और अब तक की कार्रवाइयों और अवलोकनों का एक रिकॉर्ड दिया जाता है। यह कार्रवाई उत्पन्न करने से पहले एक या अधिक विचारों को उत्पन्न करता है, जो फिर वातावरण में निष्पादित की जाती है।

[DEPS](/article/hi/DEPS) ("describe, explain, plan and select") विधि में, एक LLM पहले छवि विवरण के माध्यम से दृश्य दुनिया से जुड़ा होता है। फिर इसे अपने पहले से प्रशिक्षित ज्ञान और प्राप्त पर्यावरण प्रतिक्रिया के आधार पर जटिल कार्यों और व्यवहारों के लिए योजनाएँ तैयार करने के लिए कहा जाता है।

[Reflexion](/article/hi/Reflexion) विधि एक ऐसा एजेंट बनाती है जो कई एपिसोड में सीखता है। प्रत्येक एपिसोड के अंत में, LLM को एपिसोड का रिकॉर्ड दिया जाता है और इसे बाद के एपिसोड में बेहतर प्रदर्शन करने में मदद करने के लिए "सीखे गए सबक" सोचने के लिए कहा जाता है। ये "सीखे गए सबक" दीर्घकालिक स्मृति के एक प्रकार के रूप में संग्रहीत किए जाते हैं और बाद के एपिसोड में एजेंट को दिए जाते हैं।

खुले-अंत वाली खोज के लिए, एक LLM का उपयोग अवलोकनों के "रुचिकरता" को स्कोर करने के लिए किया जा सकता है, जिसका उपयोग एक सामान्य (गैर-LLM) रीइन्फोर्समेंट लर्निंग एजेंट को गाइड करने के लिए रिवॉर्ड सिग्नल के रूप में किया जा सकता है। वैकल्पिक रूप से, यह पाठ्यक्रम शिक्षण के लिए बढ़ती कठिनाई वाले कार्यों का प्रस्ताव कर सकता है। व्यक्तिगत कार्रवाइयों को आउटपुट करने के बजाय, एक LLM प्लानर "कौशल", या जटिल कार्रवाई अनुक्रमों के लिए फंक्शन बना सकता है। कौशल को संग्रहीत किया जा सकता है और बाद में बुलाया जा सकता है, जिससे योजना बनाने में अधिक स्तर का अमूर्तन होता है।

मेमोरी वाले कई एजेंट सामाजिक रूप से बातचीत कर सकते हैं।

### तर्क

LLM को पारंपरिक रूप से मध्यवर्ती चरणों को उत्पन्न किए बिना एक आउटपुट उत्पन्न करने के लिए प्रशिक्षित किया जाता है। नतीजतन, उनका प्रदर्शन (कम से कम मनुष्यों में) मध्यवर्ती विचार के चरणों की आवश्यकता वाले जटिल प्रश्नों पर अधिक खराब होता है। प्रारंभिक शोध ने प्रदर्शित किया कि मध्यवर्ती "स्क्रैचपैड" गणनाओं को डालने से इस तरह के कार्यों पर प्रदर्शन में सुधार हो सकता है। बाद की विधियों ने LLM के लिए कार्यों को छोटे चरणों में तोड़कर, या तो मैन्युअल रूप से या स्वचालित रूप से, इस कमी को व्यवस्थित रूप से दूर किया।

#### चेनिंग

"प्रॉम्प्ट चेनिंग" प्रतिमान 2021 में प्रकाशित किया गया था। इस विधि में, एक उपयोगकर्ता एक जटिल समस्या को मैन्युअल रूप से कई चरणों में विभाजित करता है। प्रत्येक चरण में, LLM को एक प्रॉम्प्ट मिलता है जो उसे क्या करना है बताता है और पिछले चरणों से कुछ परिणाम मिलते हैं। फिर एक चरण का परिणाम अगले चरण में फिर से उपयोग किया जाता है, जब तक एक अंतिम उत्तर नहीं मिल जाता। एक LLM की निर्देशों का पालन करने की क्षमता का अर्थ है कि कुछ परीक्षण और त्रुटि के कुछ राउंड देने पर भी गैर-विशेषज्ञ एक सफल चरण-वार प्रॉम्प्ट संग्रह लिख सकते हैं।

2022 के एक पेपर ने "चेन-ऑफ-थॉट प्रॉम्प्टिंग" नामक एक अलग तकनीक का प्रदर्शन किया, जो LLM को स्वायत्त रूप से प्रश्न को विभाजित करने की अनुमति देता है। एक LLM को कुछ उदाहरण दिए जाते हैं जहां "असिस्टेंट" अंतिम उत्तर पर पहुंचने से पहले मौखिक रूप से विचार प्रक्रिया को तोड़ता है। LLM इन उदाहरणों की नकल करता है और अंतिम उत्तर प्रदान करने से पहले कुछ मध्यवर्ती चरण उत्पन्न करने का प्रयास करता है। यह अतिरिक्त कदम, जिसे प्रॉम्प्टिंग से प्रेरित किया जाता है, LLM की अपेक्षाकृत जटिल सवालों की सटीकता में सुधार करता है। गणितीय शब्द सवालों पर, प्रॉम्प्ट किया गया मॉडल वेरिफायर के साथ फाइन-ट्यून्ड GPT-3 से भी बेहतर प्रदर्शन कर सकता है। चेन-ऑफ-थॉट को "चलो चरण-दर-चरण सोचें" जैसे निर्देश को प्रॉम्प्ट में जोड़कर भी प्रेरित किया जा सकता है, ताकि LLM को सीधे उत्तर का अनुमान लगाने की कोशिश करने के बजाय व्यवस्थित रूप से आगे बढ़ने के लिए प्रोत्साहित किया जा सके।

अनुवर्ती विधियों में स्व-निरंतरता प्रॉम्प्टिंग शामिल थी, जो कई तर्क पथों का नमूना लेती है और सबसे आम उत्तर का चयन करती है, और लीस्ट-टू-मोस्ट प्रॉम्प्टिंग, जो जटिल समस्याओं को सरल उप-समस्याओं में विघटित करती है जिन्हें मॉडल क्रमिक रूप से हल करता है।

बाद के शोध ने प्रतिबिंब का भी पता लगाया, जहां मॉडल अपने स्वयं के तर्क की आलोचना और सुधार के लिए पुनरावृत्ति करते हैं, और टूल-वर्धित तर्क, जहां मॉडल समस्या-समाधान का समर्थन करने के लिए रिट्रीवर्स या कैलकुलेटर जैसे बाहरी सिस्टम का उपयोग करते हैं।

#### मॉडल-नेटिव रीज़निंग

2024 के अंत में "तर्क मॉडल" लॉन्च किए गए थे। ये अंतिम उत्तर प्रदान करने से पहले चरण-दर-चरण समाधान उत्पन्न करने में अधिक समय बिताने के लिए प्रशिक्षित किए गए थे, जिनका उद्देश्य मानव समस्या-समाधान प्रक्रियाओं के समान होना था। OpenAI ने सितंबर 2024 में अपने o1 मॉडल के साथ इस अवधारणा का परिचय दिया, जिसके बाद अप्रैल 2025 में o3 आया। इंटरनेशनल मैथेमेटिक्स ओलंपियाड क्वालीफाइंग एग्जाम के सवालों पर, GPT-4o ने 13% सटीकता हासिल की जबकि o1 ने 83% तक पहुंच गया।

जनवरी 2025 में, चीनी कंपनी DeepSeek ने DeepSeek-R1 जारी किया, 671 अरब पैरामीटर वाला एक ओपन-वेट तर्क मॉडल जो OpenAI के o1 के तुलनीय प्रदर्शन करता है लेकिन काफी कम लागत पर संचालित होता है। OpenAI के स्वामित्व वाले मॉडल के विपरीत, DeepSeek-R1 की ओपन-वेट प्रकृति ने शोधकर्ताओं को एल्गोरिदम का अध्ययन करने और उस पर निर्माण करने की अनुमति दी, हालांकि इसका प्रशिक्षण डेटा निजी रहा।

ये तर्क मॉडल आमतौर पर पारंपरिक LLM की तुलना में प्रति क्वेरी अधिक कम्प्यूटेशनल संसाधनों की आवश्यकता होती है, क्योंकि वे समस्याओं को चरण-दर-चरण हल करने के लिए अधिक व्यापक प्रसंस्करण करते हैं।

#### इन्फरेंस ऑप्टिमाइज़ेशन

इन्फरेंस ऑप्टिमाइज़ेशन तकनीकें हैं जो मॉडल के पुनर्प्रशिक्षण की आवश्यकता के बजाय इन्फरेंस प्रक्रिया के दौरान अतिरिक्त कम्प्यूटेशनल संसाधन लागू करके LLM प्रदर्शन में सुधार करती हैं। ये दृष्टिकोण विभिन्न अत्याधुनिक तर्क और निर्णय-निर्माण रणनीतियों को लागू करते हैं ताकि सटीकता और क्षमताओं को बढ़ाया जा सके।

OptiLLM एक OpenAI API-संगत इन्फरेंस ऑप्टिमाइज़ेशन प्रॉक्सी है जो एक साथ कई इन्फरेंस ऑप्टिमाइज़ेशन तकनीकों को लागू करता है। सिस्टम किसी भी LLM प्रदाता के साथ काम कर सकने वाले पारदर्शी प्रॉक्सी के रूप में कार्य करता है, जिसमें मोंटे कार्लो ट्री सर्च (MCTS), एजेंट्स का मिश्रण (MOA), बेस्ट-ऑफ-N सैंपलिंग और चेन-ऑफ-थॉट रिफ्लेक्शन जैसी तकनीकें शामिल हैं। OptiLLM प्रदर्शित करता है कि इन्फरेंस समय पर कंप्यूटेशनल संसाधनों के रणनीतिक अनुप्रयोग से विविध कार्यों में मॉडल प्रदर्शन में काफी सुधार हो सकता है, AIME 2024 गणित प्रतियोगिता और विभिन्न कोडिंग चुनौतियों जैसे बेंचमार्क पर महत्वपूर्ण सुधार प्राप्त करके।

ये इन्फरेंस ऑप्टिमाइज़ेशन दृष्टिकोण उपकरणों की बढ़ती श्रेणी का प्रतिनिधित्व करते हैं जो मॉडल वेट्स तक पहुंच या पुनर्प्रशिक्षण की आवश्यकता के बिना मौजूदा LLM को बढ़ाते हैं, जिससे उन्नत तर्क क्षमताएं विभिन्न मॉडल प्रदाताओं और उपयोग के मामलों में अधिक सुलभ हो जाती हैं।

## इनपुट और आउटपुट के रूप

### बहुसंवेदिता

बहुसंवेदिता का अर्थ है कई मोडैलिटी होना, जहां "मोडैलिटी" एक प्रकार का इनपुट या आउटपुट है, जैसे वीडियो, इमेज, ऑडियो, टेक्स्ट, प्रोप्रिओसेप्शन आदि। उदाहरण के लिए, Google PaLM मॉडल को फाइन-ट्यून किया गया और एक बहुसंवेदी मॉडल में रोबोटिक नियंत्रण पर लागू किया गया। LLaMA मॉडल को भी टोकनाइज़ेशन विधि का उपयोग करके बहुसंवेदी बनाया गया है, ताकि इमेज इनपुट और वीडियो इनपुट की अनुमति मिल सके। [GPT-4o](/article/hi/GPT-4o) टेक्स्ट, ऑडियो और इमेज को प्रोसेस और जनरेट कर सकता है। ऐसे मॉडलों को कभी-कभी बड़े बहुसंवेदी मॉडल (LMMs) कहा जाता है।

एक LLM से बहुसंवेदी मॉडल बनाने की एक आम विधि एक प्रशिक्षित एनकोडर के आउटपुट को "टोकनाइज" करना है। विशेष रूप से, इमेज को समझने वाले LLM का निर्माण इस प्रकार किया जा सकता है: एक प्रशिक्षित LLM लें, और एक प्रशिक्षित इमेज एनकोडर 
E
 लें। एक छोटा मल्टीलेयर पर्सेप्ट्रॉन f बनाएं, ताकि किसी भी इमेज y के लिए, पोस्ट-प्रोसेस किया गया वेक्टर f(E(y)) का आयाम एक एनकोडेड टोकन के समान हो। यह एक "इमेज टोकन" है। फिर, टेक्स्ट टोकन और इमेज टोकन को इंटरलीव किया जा सकता है। फिर कंपाउंड मॉडल को इमेज-टेक्स्ट डेटासेट पर फाइन-ट्यून किया जाता है। मॉडल को बेहतर बनाने के लिए इस बुनियादी निर्माण को अधिक परिष्कार के साथ लागू किया जा सकता है। इमेज एनकोडर को स्थिरता बढ़ाने के लिए फ्रीज किया जा सकता है। इस प्रकार की विधि, जहां कई मोडैलिटी से एम्बेडिंग को फ़्यूज़ किया जाता है और प्रेडिक्टर को संयुक्त एम्बेडिंग पर प्रशिक्षित किया जाता है, को अर्ली फ़्यूज़न कहा जाता है।

एक अन्य विधि, जिसे इंटरमीडिएट फ़्यूज़न कहा जाता है, में प्रत्येक मोडैलिटी को पहले स्वतंत्र रूप से संसाधित किया जाता है ताकि मोडैलिटी-विशिष्ट प्रतिनिधित्व प्राप्त किया जा सके; फिर इन मध्यवर्ती प्रतिनिधित्वों को एक साथ फ़्यूज़ किया जाता है। सामान्य तौर पर, क्रॉस-अटेंशन का उपयोग विभिन्न मोडैलिटी से जानकारी को एकीकृत करने के लिए किया जाता है। उदाहरण के लिए, Flamingo मॉडल क्रॉस-अटेंशन परतों का उपयोग अपने पूर्व-प्रशिक्षित भाषा मॉडल में विज़ुअल जानकारी डालने के लिए करता है।

### गैर-प्राकृतिक भाषाएँ

LLM प्रोग्रामिंग भाषाओं को इसी तरह से संभाल सकते हैं जैसे वे प्राकृतिक भाषाओं को संभालते हैं। कोड, मानव भाषा की तरह, सादे पाठ के रूप में प्रस्तुत किया जाता है, इसलिए टोकन हैंडलिंग में कोई विशेष परिवर्तन की आवश्यकता नहीं है। LLM प्राकृतिक भाषा में लिखी समस्याओं या निर्देशों के आधार पर कोड जनरेट कर सकते हैं। वे प्राकृतिक भाषा में कोड का वर्णन भी कर सकते हैं या प्रोग्रामिंग भाषाओं के बीच अनुवाद कर सकते हैं। वे मूल रूप से एक कोड कंप्लीशन टूल के रूप में उपयोग किए जाते थे, लेकिन प्रगति ने उन्हें स्वचालित प्रोग्रामिंग की ओर ले जाया है। [GitHub Copilot](/article/hi/GitHub_Copilot) जैसी सेवाएँ प्रोग्रामिंग के लिए विशेष रूप से प्रशिक्षित, फाइन-ट्यून या प्रॉम्प्टेड LLM प्रदान करती हैं।

LLM आर्किटेक्चर जैविक अनुक्रमों के विश्लेषण में भी उपयोगी साबित हुए हैं: प्रोटीन, डीएनए और आरएनए। प्रोटीन के साथ, वे एमिनो एसिड अनुक्रम से "व्याकरण" के एक डिग्री को कैप्चर करने में सक्षम दिखाई देते हैं, एक अनुक्रम को एक एम्बेडिंग में संघनित करते हैं। संरचना भविष्यवाणी और म्यूटेशनल आउटकम भविष्यवाणी जैसे कार्यों पर, एम्बेडिंग का उपयोग करने वाला एक छोटा मॉडल कई अनुक्रम संरेखण (MSA) का उपयोग करने वाले बहुत बड़े मॉडलों के बराबर या उनसे आगे जा सकता है। ESMFold, Meta Platforms का एम्बेडिंग-आधारित प्रोटीन संरचना भविष्यवाणी विधि, MSA आवश्यकता के हटने और एम्बेडिंग के उपयोग के कारण पैरामीटर संख्या के कम होने के कारण AlphaFold2 से एक क्रम तेज़ी से चलता है। Meta ESM Atlas होस्ट करता है, ESMFold का उपयोग करके भविष्यवाणी की गई 772 मिलियन मेटाजेनोमिक प्रोटीन संरचनाओं का एक डेटाबेस। एक LLM प्रकृति में देखे गए किसी भी प्रोटीन के विपरीत प्रोटीन डिजाइन भी कर सकता है। न्यूक्लिक एसिड मॉडल रेगुलेटरी अनुक्रमों की पहचान, अनुक्रम वर्गीकरण, RNA-RNA इंटरैक्शन भविष्यवाणी और RNA संरचना भविष्यवाणी में उपयोगी साबित हुए हैं।

## विशेषताएँ

### स्केलिंग लॉज़

प्री-ट्रेनिंग के बाद LLM का प्रदर्शन मुख्य रूप से निम्न पर निर्भर करता है:

प्री-ट्रेनिंग की लागत 
C
 (उपयोग किए गए कुल कम्प्यूट की मात्रा),
कृत्रिम न्यूरल नेटवर्क के आकार, जैसे पैरामीटर की संख्या 
N
 (यानी परतों में न्यूरॉन्स की मात्रा, उनके बीच वेट्स और बायस),
प्री-ट्रेनिंग डेटासेट का आकार (यानी कॉर्पस में टोकन की संख्या, 
D
)।
"स्केलिंग लॉज़" ऐसे अनुभवजन्य सांख्यिकीय नियम हैं जो इस तरह के कारकों के आधार पर LLM प्रदर्शन का अनुमान लगाते हैं। LLM के लिए एक विशेष स्केलिंग नियम ("चिंचिला स्केलिंग") जो एक epoch के लिए ऑटोरेग्रेसिव रूप से प्रशिक्षित है, लॉग-लॉग लर्निंग रेट शेड्यूल के साथ, कहता है:

  
C = C₀ND
L = A/Nᵅ + B/Dᵝ + L₀
  

जहां चर हैं:

C मॉडल प्रशिक्षण की लागत है, FLOP में।
N मॉडल में पैरामीटर की संख्या है।
D प्रशिक्षण सेट में टोकन की संख्या है।
L परीक्षण डेटासेट पर प्रशिक्षित LLM द्वारा प्राप्त प्रति टोकन औसत नकारात्मक लॉग-संभाव्यता हानि (nats/token) है।
और सांख्यिकीय हाइपरपैरामीटर हैं:

C₀ = 6, यानी एक टोकन पर प्रशिक्षण की लागत प्रति पैरामीटर 6 FLOP है। ध्यान दें कि प्रशिक्षण लागत अनुमान लागत से बहुत अधिक है, जहां एक टोकन का अनुमान लगाने की लागत प्रति पैरामीटर 1 से 2 FLOP है।
α = 0.34, β = 0.28, A = 406.4, B = 410.7, L₀ = 1.69

### उभरती क्षमताएँ

विभिन्न कार्यों पर बड़े मॉडलों का प्रदर्शन, जब लॉग-लॉग पैमाने पर प्लॉट किया जाता है, तो छोटे मॉडलों द्वारा प्राप्त प्रदर्शन के एक रैखिक एक्सट्रापोलेशन के रूप में दिखाई देता है। हालांकि, यह रैखिकता स्केलिंग नियम में अचानक "ब्रेक" से टूट सकती है, जहां रेखा का ढलान अचानक बदल जाता है, और बड़े मॉडल "उभरती क्षमताएँ" प्राप्त करते हैं। ये क्षमताएं मॉडल के घटकों के जटिल इंटरैक्शन से उत्पन्न होती हैं और स्पष्ट रूप से प्रोग्राम या डिज़ाइन नहीं की जाती हैं।

उभरती क्षमताओं में से एक उदाहरण उदाहरणों से इन-कॉन्टेक्स्ट लर्निंग है। इन-कॉन्टेक्स्ट लर्निंग ऐसे कार्यों में शामिल है, जैसे:

रिपोर्टेड अंकगणित
अंतर्राष्ट्रीय ध्वनि वर्णमाला को डिकोड करना
एक शब्द के अक्षरों को अनस्क्रैम्बल करना
शब्द-इन-कॉन्टेक्स्ट डेटासेट में अस्पष्टता दूर करना
स्पेशल शब्दों का रूपांतरण
कार्डिनल दिशाएँ (उदाहरण के लिए, शीर्ष-दाएँ में एक 1 और 8 शून्य के साथ 3x3 ग्रिड के जवाब में "उत्तर-पूर्व" कहना), टेक्स्ट में प्रस्तुत रंग के शब्द।
चेन-ऑफ-थॉट प्रॉम्प्टिंग: 2022 के शोध पेपर में, चेन-ऑफ-थॉट प्रॉम्प्टिंग ने केवल उन मॉडलों के प्रदर्शन में सुधार किया जिनमें कम से कम 62B पैरामीटर थे। छोटे मॉडल बेहतर प्रदर्शन करते हैं जब उन्हें चेन ऑफ थॉट के बिना, तुरंत उत्तर देने के लिए कहा जाता है।
हिंगलिश (हिंदी और अंग्रेजी के संयोजन) के पैराग्राफ में आपत्तिजनक सामग्री की पहचान करना, और स्वाहिली कहावतों के अंग्रेजी समकक्ष उत्पन्न करना।
शेफर और अन्य का तर्क है कि उभरती क्षमताएँ अप्रत्याशित रूप से प्राप्त नहीं की जाती हैं, बल्कि सुचारू स्केलिंग नियम के अनुसार अनुमानित रूप से प्राप्त होती हैं। लेखकों ने बहु-विकल्प प्रश्नों को हल करने वाले LLM के एक खिलौना सांख्यिकीय मॉडल पर विचार किया, और दिखाया कि यह सांख्यिकीय मॉडल, अन्य प्रकार के कार्यों के लिए संशोधित, इन कार्यों पर भी लागू होता है।

## व्याख्या

### मैकेनिस्टिक इंटरप्रिटेबिलिटी

मैकेनिस्टिक इंटरप्रिटेबिलिटी का उद्देश्य LLM के भीतर व्यक्तिगत न्यूरॉन्स या सर्किट की सटीक पहचान और समझ करना है जो विशिष्ट व्यवहारों या आउटपुट का उत्पादन करते हैं। मॉडल घटकों को अणु स्तर पर रिवर्स-इंजीनियरिंग करके, शोधकर्ता तैनाती से पहले उभरते हानिकारक व्यवहारों, पूर्वाग्रहों, धोखे या अनपेक्षित लक्ष्य खोज जैसे सुरक्षा चिंताओं का पता लगाने और उन्हें कम करने का लक्ष्य रखते हैं। मैकेनिस्टिक इंटरप्रिटेबिलिटी अनुसंधान [Anthropic](/article/hi/Anthropic) और OpenAI जैसे संगठनों में किया गया है, हालांकि LLM की आंतरिक कार्यप्रणाली को समझना अभी भी कठिन बना हुआ है।

मैकेनिस्टिक इंटरप्रिटेबिलिटी ने बड़े भाषा मॉडलों को अपरिवर्तनीय "ब्लैक बॉक्स" के रूप में चित्रित करने की जगह क्रमिक रूप से उन न्यूरॉन्स और सर्किट की पहचान करके ली है जो विशिष्ट गणना को लागू करते हैं और यह ट्रैस करते हैं कि प्रतिनिधित्व ट्रांसफॉर्मर परतों के माध्यम से कैसे प्रचारित होते हैं। शोधकर्ताओं ने स्वचालित न्यूरॉन-व्याख्या पाइपलाइन का प्रदर्शन किया है और न्यूरॉन-स्तरीय डेटासेट जारी किए हैं, और उन्होंने सर्किट-ट्रेसिंग और रिप्लेसमेंट-मॉडल विधियों को विकसित किया है जो एट्रिब्यूशन ग्राफ और कंपोनेंट-स्तरीय विवरण उत्पन्न करते हैं जो आधुनिक ट्रांसफॉर्मर मॉडल पर लागू होते हैं।

पर्याप्त सीमाएँ बनी हुई हैं, जिनमें पॉलिसेमेंटिसिटी, सुपरपोजिशन, प्रतिस्पर्धी स्पष्टीकरणों की गैर-पहचान, और मानवीय अनुमान के जोखिम शामिल हैं, इसलिए वर्तमान मैकेनिस्टिक परिणाम नियंत्रणीयता में सुधार करते हैं और कार्रवाई योग्य हस्तक्षेपों को सामने लाते हैं। ये परिणाम अपने आप में मानव मस्तिष्क या मानव मन के मॉडल के रूप में LLM के साथ व्यवहार करने का औचित्य नहीं देते हैं, बिना अतिरिक्त अनुभवजन्य सत्यापन और अंतर-अनुशासनात्मक सबूत के। थिंकिंग मशीन्स लैब ने LLM इन्फरेंस में गैर-डिटर्मिनिज्म को हराने के लिए पुनरुत्पादक इंटरप्रिटेबिलिटी कार्य प्रकाशित किया है।

रिवर्स-इंजीनियरिंग LLM द्वारा किए गए अनुमानों का अनुमान लगाने वाले एल्गोरिदम की खोज का नेतृत्व कर सकती है। उदाहरण के लिए, लेखकों ने मॉड्यूलर अंकगणितीय जोड़ पर छोटे ट्रांसफॉर्मर को प्रशिक्षित किया। परिणामी मॉडलों को रिवर्स-इंजीनियर किया गया था, और यह पता चला कि वे डिस्क्रीट फूरियर ट्रांसफॉर्म का उपयोग कर रहे थे। मॉडल के प्रशिक्षण ने ग्रोकिंग नामक घटना पर भी प्रकाश डाला, जिसमें मॉडल शुरू में प्रशिक्षण सेट में सभी संभावित परिणामों को याद करता है (ओवरफिटिंग), और बाद में अचानक वास्तव में गणना करना सीखता है।

LLM की पारदर्शिता और व्याख्येयता को बढ़ाने के लिए कुछ तकनीकें विकसित की गई हैं। ट्रांसकोडर, जो ट्रांसफॉर्मर की तुलना में अधिक व्याख्येय हैं, का उपयोग "रिप्लेसमेंट मॉडल" विकसित करने के लिए किया गया है। एक ऐसे अध्ययन में जिसमें LLM द्वारा तुकबंदी वाली कविता लिखने की मैकेनिस्टिक व्याख्या शामिल थी, यह दिखाया गया था कि हालांकि उनका मानना है कि वे बस अगले टोकन की भविष्यवाणी करते हैं, वे वास्तव में, आगे की योजना बना सकते हैं। ऐसी तकनीकों को एकीकृत करके, शोधकर्ता और प्रैक्टिशनर LLM के संचालन में गहरी अंतर्दृष्टि प्राप्त कर सकते हैं, विश्वास को बढ़ावा दे सकते हैं और इन शक्तिशाली मॉडलों की जिम्मेदार तैनाती की सुविधा प्रदान कर सकते हैं।

### समझ और बुद्धिमत्ता

NLP शोधकर्ता 2022 के एक सर्वेक्षण में पूछे जाने पर समान रूप से विभाजित थे कि क्या (अनट्यून्ड) LLM "किसी भी महत्वपूर्ण अर्थ में प्राकृतिक भाषा को समझ सकते हैं"। "LLM समझ" के समर्थकों का मानना है कि गणितीय तर्क जैसी कुछ LLM क्षमताएँ कुछ अवधारणाओं को "समझने" की क्षमता का संकेत देती हैं। Microsoft की एक टीम ने 2023 में तर्क दिया कि GPT-4 "गणित, कोडिंग, विज़न, चिकित्सा, कानून, मनोविज्ञान और अधिक को कवर करने वाले नवीन और कठिन कार्यों को हल कर सकता है" और कि GPT-4 को "एक कृत्रिम सामान्य बुद्धिमत्ता प्रणाली के प्रारंभिक (हालांकि अभी भी अपूर्ण) संस्करण के रूप में देखा जा सकता है": "क्या कोई तर्कसंगत रूप से कह सकता है कि एक सिस्टम जो सॉफ्टवेयर इंजीनियरिंग उम्मीदवारों के लिए परीक्षा पास करता है, वास्तव में बुद्धिमान नहीं है?" [Ilya Sutskever](/article/hi/Ilya_Sutskever) का तर्क है कि कभी-कभी अगले शब्द की भविष्यवाणी करने में तर्क और गहरी अंतर्दृष्टि शामिल होती है, उदाहरण के लिए अगर LLM को पूरी कहानी के प्रसंस्करण के बाद एक अज्ञात डिटेक्टिव उपन्यास में अपराधी के नाम की भविष्यवाणी करनी हो। कुछ शोधकर्ता LLM को "विदेशी बुद्धिमत्ता" के रूप में वर्णित करते हैं। उदाहरण के लिए, Conjecture CEO Connor Leahy अनट्यून्ड LLM को अबोधगम्य विदेशी "[Shoggoths](/article/hi/Shoggoths)" के समान मानते हैं, और मानते हैं कि RLHF ट्यूनिंग LLM के आंतरिक कार्य पर एक "मुस्कुराता चेहरा" बनाती है: "अगर आप इसे बहुत दूर नहीं धकेलते हैं, तो मुस्कुराता चेहरा बना रहता है। लेकिन फिर आप इसे [अप्रत्याशित] प्रॉम्प्ट देते हैं, और अचानक आप पागलपन के इस विशाल अंडरबेली, अजीब सोच प्रक्रियाओं और स्पष्ट रूप से गैर-मानवीय समझ को देखते हैं।"

इसके विपरीत, LLM समझ के आलोचकों का मानना है कि मौजूदा LLM "बस मौजूदा लेखन को रीमिक्स और रीकंबाइन कर रहे हैं", एक घटना जिसे [स्टोकैस्टिक पैरट](/article/hi/स्टोकैस्टिक_पैरट) कहा जाता है, या वे भविष्यवाणी कौशल, तर्क कौशल, एजेंसी और स्पष्टीकरण में मौजूदा LLM की चल रही कमियों की ओर इशारा करते हैं। उदाहरण के लिए, GPT-4 में योजना बनाने और रीयल-टाइम सीखने में प्राकृतिक कमियां हैं। जेनरेटिव LLM को तथ्यात्मक दावों का आत्मविश्वास से दावा करते हुए देखा गया है जो उनके प्रशिक्षण डेटा द्वारा समर्थित नहीं लगते हैं, एक घटना जिसे "[हैलुसिनेशन](/article/hi/LLM_हैलुसिनेशन)" कहा गया है। विशेष रूप से, LLM के संदर्भ में हैलुसिनेशन पाठ या प्रतिक्रियाओं के उत्पादन को संदर्भित करता है जो वाक्यगत रूप से सही, प्रवाहपूर्ण और प्राकृतिक लगता है लेकिन तथ्यात्मक रूप से गलत, बेतुका या प्रदान किए गए स्रोत इनपुट के लिए अविश्वसनीय है। न्यूरोसाइंटिस्ट [Terrence Sejnowski](/article/hi/Terrence_Sejnowski) ने तर्क दिया है कि "LLM की बुद्धिमत्ता पर विशेषज्ञों के मतभेद बताते हैं कि प्राकृतिक बुद्धिमत्ता पर आधारित हमारे पुराने विचार अपर्याप्त हैं"।

हैलुसिनेशन को कम करने या उसके लिए क्षतिपूर्ति करने के प्रयासों ने स्वचालित तर्क, रिट्रीवल-ऑग्मेंटेड जनरेशन (RAG), फाइन-ट्यूनिंग और अन्य विधियों का उपयोग किया है।

LLM के बुद्धिमत्ता या समझ प्रदर्शित करने के मामले के दो मुख्य पहलू हैं - पहला यह है कि कंप्यूटर सिस्टम में विचार और भाषा को कैसे मॉडल किया जाए, और दूसरा यह है कि कंप्यूटर सिस्टम को मानव जैसी भाषा उत्पन्न करने में कैसे सक्षम बनाया जाए। भाषा के एक मॉडल के रूप में संज्ञान के ये पहलू [संज्ञानात्मक भाषाविज्ञान](/article/hi/संज्ञानात्मक_भाषाविज्ञान) के क्षेत्र में विकसित किए गए हैं। अमेरिकी भाषाविद् [जॉर्ज लेकॉफ](/article/hi/जॉर्ज_लेकॉफ) ने न्यूरल थ्योरी ऑफ़ लैंग्वेज (NTL) को सीखने के कार्यों और समझ के मॉडल के रूप में भाषा के उपयोग के लिए एक कम्प्यूटेशनल आधार के रूप में प्रस्तुत किया। NTL मॉडल रेखांकित करता है कि मानव मस्तिष्क के विशिष्ट न्यूरल संरचनाएँ कैसे विचार और भाषा की प्रकृति को आकार देती हैं और बदले में ऐसी न्यूरल प्रणालियों के कम्प्यूटेशनल गुण क्या हैं जिन्हें कंप्यूटर सिस्टम में विचार और भाषा को मॉडल करने के लिए लागू किया जा सकता है। कंप्यूटर सिस्टम में भाषा को मॉडल करने के लिए एक फ्रेमवर्क स्थापित करने के बाद, ध्यान कंप्यूटर सिस्टम को स्वीकार्य व्याकरण के साथ मानव जैसी भाषा उत्पन्न करने में सक्षम बनाने के लिए फ्रेमवर्क स्थापित करने पर केंद्रित हो गया। अपनी 2014 की पुस्तक "[द लैंग्वेज मिथ: वाई लैंग्वेज इज़ नॉट एन इंस्टिंक्ट](/article/hi/द_लैंग्वेज_मिथ)" में, ब्रिटिश संज्ञानात्मक भाषाविद् और डिजिटल संचार प्रौद्योगिकीविद् [Vyvyan Evans](/article/hi/Vyvyan_Evans) ने [प्रोबैबिलिस्टिक कॉन्टेक्स्ट-फ्री ग्रामर](/article/hi/प्रोबैबिलिस्टिक_कॉन्टेक्स्ट-फ्री_ग्रामर) (PCFG) की भूमिका का मानचित्रण किया, जो NLP को संज्ञानात्मक पैटर्न का मॉडल बनाने और मानव जैसी भाषा उत्पन्न करने में सक्षम बनाता है।

## मूल्यांकन

### परप्लेक्सिटी

किसी भी भाषा मॉडल के प्रदर्शन का मानक माप दिए गए पाठ कॉर्पस पर इसकी परप्लेक्सिटी है। परप्लेक्सिटी यह मापती है कि मॉडल किसी डेटासेट की सामग्री का कितनी अच्छी तरह अनुमान लगाता है; मॉडल डेटासेट को जितनी अधिक संभावना देता है, परप्लेक्सिटी उतनी ही कम होती है। गणितीय शब्दों में, परप्लेक्सिटी प्रति टोकन औसत नकारात्मक लॉग संभावना का घातीय है।

  log(Perplexity) = −(1/N) Σ(i=1 to N) log(Pr(tokenᵢ|context for tokenᵢ))

जहां, N पाठ कॉर्पस में टोकन की संख्या है, और "टोकन i के लिए प्रसंग" विशेष प्रकार के LLM पर निर्भर करता है। यदि LLM ऑटोरेग्रेसिव है, तो "टोकन i के लिए प्रसंग" टोकन i से पहले दिखाई देने वाला पाठ का एक खंड है। यदि LLM मास्क्ड है, तो "टोकन i के लिए प्रसंग" टोकन i के आसपास का पाठ खंड है।

चूंकि भाषा मॉडल प्रशिक्षण डेटा पर ओवरफिट कर सकते हैं, इसलिए मॉडलों का मूल्यांकन आमतौर पर एक परीक्षण सेट पर उनकी परप्लेक्सिटी से किया जाता है। यह मूल्यांकन बड़े मॉडलों के लिए संभावित रूप से समस्याग्रस्त है जो, क्योंकि वे बढ़ते पाठ कॉर्पस पर प्रशिक्षित होते हैं, अनजाने में किसी दिए गए परीक्षण सेट के हिस्सों को शामिल करने की अधिक संभावना रखते हैं।

### मापदंड

सूचना सिद्धांत में, एन्ट्रॉपी की अवधारणा परप्लेक्सिटी से निकटता से जुड़ी हुई है, एक संबंध जिसे उल्लेखनीय रूप से क्लाउड शैनन द्वारा स्थापित किया गया था। यह संबंध गणितीय रूप से Entropy = log₂(Perplexity) के रूप में व्यक्त किया जाता है।

इस संदर्भ में, एन्ट्रॉपी को आमतौर पर प्रति शब्द बिट्स (BPW) या प्रति वर्ण बिट्स (BPC) के रूप में मापा जाता है, जो इस बात पर निर्भर करता है कि भाषा मॉडल शब्द-आधारित या वर्ण-आधारित टोकनाइज़ेशन का उपयोग करता है।

विशेष रूप से, बड़े भाषा मॉडलों के मामले में जो प्रमुख रूप से सब-वर्ड टोकनाइज़ेशन का उपयोग करते हैं, प्रति टोकन बिट्स (BPT) एक अधिक उपयुक्त माप प्रतीत होता है। हालांकि, विभिन्न LLM के बीच टोकनाइज़ेशन पद्धतियों में भिन्नता के कारण, BPT विविध मॉडलों के बीच तुलनात्मक विश्लेषण के लिए एक विश्वसनीय मेट्रिक नहीं है। BPT को BPW में बदलने के लिए, इसे प्रति शब्द औसत टोकन की संख्या से गुणा किया जा सकता है।

भाषा मॉडलों के मूल्यांकन और तुलना में, क्रॉस-एन्ट्रॉपी आम तौर पर एन्ट्रॉपी की तुलना में पसंदीदा मेट्रिक है। अंतर्निहित सिद्धांत यह है कि एक निम्न BPW मॉडल की बेहतर संपीड़न क्षमता का संकेत देता है। यह, बदले में, सटीक भविष्यवाणियां करने के लिए मॉडल की प्रवीणता को दर्शाता है।

अपनी टोकन को सटीक रूप से अनुमानित करने की क्षमता के कारण, LLM लॉसलेस कंप्रेशन में अत्यधिक सक्षम हैं। DeepMind के 2023 के एक अध्ययन ने दिखाया कि Chinchilla मॉडल, हालांकि मुख्य रूप से पाठ पर प्रशिक्षित, ImageNet को इसके आकार के 43% तक संपीड़ित करने में सक्षम था, जो 58% के साथ PNG को पीछे छोड़ दिया।

### बेंचमार्क

बेंचमार्क का उपयोग विशिष्ट कार्यों पर LLM प्रदर्शन का मूल्यांकन करने के लिए किया जाता है। परीक्षण सामान्य ज्ञान, पूर्वाग्रह, सामान्य बुद्धि तर्क, प्रश्न उत्तर और गणितीय समस्या-समाधान जैसी क्षमताओं का मूल्यांकन करते हैं। कम्पोजिट बेंचमार्क कई क्षमताओं की जांच करते हैं। परिणाम अक्सर प्रॉम्प्टिंग विधि के प्रति संवेदनशील होते हैं।

एक प्रश्न-उत्तर बेंचमार्क को "ओपन बुक" माना जाता है यदि मॉडल के प्रॉम्प्ट में ऐसा पाठ शामिल है जिससे अपेक्षित उत्तर प्राप्त किया जा सकता है (उदाहरण के लिए, पिछले प्रश्न को टेक्स्ट के साथ जोड़ा जा सकता है जिसमें वाक्य शामिल है "शार्क्स ने स्टेनली कप फाइनल में एक बार प्रवेश किया, 2016 में पिट्सबर्ग पेंगुइंस से हारकर।")। अन्यथा, कार्य को "क्लोज़्ड बुक" माना जाता है, और मॉडल को केवल अपने प्रशिक्षण से ज्ञान का उपयोग करना होगा। उदाहरणों में [GLUE](/article/hi/GLUE), [SuperGLUE](/article/hi/SuperGLUE), [MMLU](/article/hi/MMLU), [BIG-bench](/article/hi/BIG-bench), [HELM](/article/hi/HELM), और HLE (ह्यूमैनिटीज़ लास्ट एग्जाम) शामिल हैं।

LLM पूर्वाग्रह का आकलन [CrowS-Pairs](/article/hi/CrowS-Pairs) (क्राउडसोर्स्ड स्टीरियोटाइप पेयर्स), [Stereo Set](/article/hi/Stereo_Set), और [Parity Benchmark](/article/hi/Parity_Benchmark) जैसे बेंचमार्क के माध्यम से किया जा सकता है।

तथ्य-जांच और भ्रामक जानकारी पहचान बेंचमार्क उपलब्ध हैं। 2023 के एक अध्ययन ने ChatGPT 3.5 और 4.0, Bard, और Bing AI सहित LLM की तथ्य-जांच सटीकता की तुलना [PolitiFact](/article/hi/PolitiFact) और [Snopes](/article/hi/Snopes) जैसे स्वतंत्र तथ्य-परीक्षकों से की। परिणामों ने मध्यम प्रवीणता प्रदर्शित की, जिसमें GPT-4 ने 71% की उच्चतम सटीकता हासिल की, जो मानव तथ्य-परीक्षकों से पीछे रह गई।

एक पहले का मानक मूल्यांकन डेटासेट के एक हिस्से का उपयोग करके परीक्षण किया गया था। प्रॉम्प्टिंग तकनीकों के माध्यम से सीधे एक प्री-ट्रेंड मॉडल का मूल्यांकन करना अधिक आम हो गया है। शोधकर्ता विशिष्ट कार्यों के लिए प्रॉम्प्ट्स को कैसे तैयार करते हैं, इसमें भिन्नता होती है, विशेष रूप से प्रॉम्प्ट से जुड़े सही उदाहरणों की संख्या के संबंध में (यानी n-शॉट प्रॉम्प्टिंग में n का मान)।

### डेटासेट

विशिष्ट डेटासेट प्रश्न और सही उत्तर के जोड़े से बने होते हैं, उदाहरण के लिए, ("क्या सैन जोस शार्क्स ने स्टेनली कप जीता है?", "नहीं")। सामान्यतः उपयोग किए जाने वाले प्रश्न-उत्तर डेटासेट के कुछ उदाहरणों में [TruthfulQA](/article/hi/TruthfulQA), [Web Questions](/article/hi/Web_Questions), [TriviaQA](/article/hi/TriviaQA), और [SQuAD](/article/hi/SQuAD) शामिल हैं।

मूल्यांकन डेटासेट पाठ पूर्णता के रूप में भी हो सकते हैं, जिसमें मॉडल को एक प्रॉम्प्ट को पूरा करने के लिए सबसे संभावित शब्द या वाक्य का चयन करना होता है, उदाहरण के लिए: "एलिस बॉब से मित्र थी। एलिस अपने मित्र से मिलने गई, ____"।

डेटासेट विभिन्न गुणवत्ता के हो सकते हैं और उनमें ऐसे प्रश्न हो सकते हैं जिन्हें गलत लेबल किया गया है, अस्पष्ट, अनुत्तरदायी या अन्यथा निम्न-गुणवत्ता वाले हो सकते हैं।

### प्रतिद्वंद्वी मूल्यांकन

LLM में तेज़ी से सुधार ने नियमित रूप से बेंचमार्क को अप्रचलित कर दिया है, जिससे मॉडल मानव एनोटेटर्स के प्रदर्शन को पार कर गए हैं। इसके अतिरिक्त, "शॉर्टकट लर्निंग" AI को बहु-विकल्प परीक्षणों पर "धोखा" देने की अनुमति देता है, जिसमें सतही परीक्षा प्रश्न शब्दों में सांख्यिकीय संबंधों का उपयोग करके सही प्रतिक्रिया का अनुमान लगाता है, बिना विशिष्ट प्रश्न पर विचार किए।

कुछ डेटासेट प्रतिद्वंद्वी हैं, जो ऐसी समस्याओं पर ध्यान केंद्रित करते हैं जो LLM को भ्रमित करते हैं। एक उदाहरण [TruthfulQA](/article/hi/TruthfulQA) डेटासेट है, 817 प्रश्नों का एक प्रश्न-उत्तर डेटासेट जो प्रशिक्षण के दौरान उजागर झूठ की नकल करके LLM को भ्रमित करता है। उदाहरण के लिए, एक LLM "क्या आप एक पुराने कुत्ते को नई चालें सिखा सकते हैं?" के प्रश्न का उत्तर "नहीं" दे सकता है क्योंकि इसे अंग्रेज़ी मुहावरे you can't teach an old dog new tricks से अवगत कराया गया था, भले ही यह शाब्दिक रूप से सच नहीं है।

प्रतिद्वंद्वी मूल्यांकन डेटासेट का एक और उदाहरण [Swag](/article/hi/Swag) और इसका उत्तराधिकारी, [HellaSwag](/article/hi/HellaSwag) है, समस्याओं का एक संग्रह जिसमें पाठ को पूरा करने के लिए कई विकल्पों में से एक का चयन करना होता है। गलत विकल्प भाषा मॉडल से नमूना लेकर उत्पन्न किए गए थे। परिणामी समस्याएं मनुष्यों के लिए सरल हैं लेकिन LLM को हरा देती हैं। उदाहरण प्रश्न:

हम एक फिटनेस केंद्र का संकेत देखते हैं। फिर हम एक व्यक्ति को कैमरे से बात करते हुए और एक व्यायाम गेंद पर बैठते और लेटते हुए देखते हैं। वह आदमी...

गेंदों पर ऊपर और नीचे दौड़ कर कुशल व्यायाम कार्य कैसे बढ़ाया जाए, यह प्रदर्शित करता है।
अपनी सभी बाहों और पैरों को हिलाता है और बहुत सारी मांसपेशियां बनाता है।
फिर गेंद खेलता है और हम एक ग्राफिक्स और हेज ट्रिमिंग प्रदर्शन देखते हैं।
गेंद पर सिट-अप्स करते हुए और बात करते हुए।

BERT 2) को सबसे संभावित पूर्णता के रूप में चुनता है, हालांकि सही उत्तर 4) है।

## सुरक्षा

AI सुरक्षा एक पेशेवर अनुशासन के रूप में मॉडल आर्किटेक्चर, प्रशिक्षण डेटा और तैनाती शासन में परिचालन जोखिमों की व्यवस्थित पहचान और कम करने को प्राथमिकता देता है, और यह ऐसे मीडिया फ्रेमवर्क्स के बजाय इंजीनियरिंग और नीति हस्तक्षेपों पर जोर देता है जो अटकलबाजी वाले अस्तित्वगत परिदृश्यों को प्रमुखता से दिखाते हैं। 2025 तक, प्रॉम्प्ट इंजेक्शन उपभोक्ताओं और व्यवसायों के लिए एक महत्वपूर्ण जोखिम है जो उनके निजी डेटा तक पहुंच वाली एजेंटिक सुविधाओं का उपयोग करते हैं।

शोधकर्ता ठोस विफलता मोड पर ध्यान केंद्रित करते हैं, जिनमें मेमोराइजेशन और कॉपीराइट लीकेज, प्रॉम्प्ट इंजेक्शन जैसे सुरक्षा एक्सप्लोइट्स, स्टीरियोटाइपिंग के रूप में प्रकट होने वाला एल्गोरिदमिक पूर्वाग्रह, डेटासेट चयन प्रभाव, और राजनीतिक झुकाव, बड़े पैमाने पर प्रशिक्षण की उच्च ऊर्जा और कार्बन लागत को कम करने के तरीके, और उपयोगकर्ताओं पर वार्तालापी एजेंटों के मापने योग्य संज्ञानात्मक और मानसिक स्वास्थ्य प्रभाव शामिल हैं, जबकि मशीन सेंटियंस के दावों के बारे में अनुभवजन्य और नैतिक अनिश्चितता को संलग्न करते हैं, और डेटासेट क्यूरेशन, इनपुट सैनिटाइज़ेशन, मॉडल ऑडिटिंग, स्केलेबल ओवरसाइट और गवर्नेंस फ्रेमवर्क जैसे शमन उपायों को लागू करते हैं।

### CBRN और कंटेंट दुरुपयोग

फ्रंटियर AI लैब CBRN (रासायनिक, जैविक, रेडियोलॉजिकल, और परमाणु रक्षा) और समान द्विउपयोगी खतरों को उच्च-परिणाम वाले दुरुपयोग के रूप में मानते हैं और परत वाले जोखिम शासन लागू करते हैं, जो क्षमता सीमाओं, पूर्व-तैनाती मूल्यांकन, प्रतिद्वंद्वी रेड-टीमिंग, सख्त एक्सेस नियंत्रण और स्पष्ट उपयोग प्रतिबंधों को जोड़कर दोनों आकस्मिक और दुर्भावनापूर्ण सहायता को सीमित करते हैं।

परिचालन उपायों में क्षमता गेटिंग और चरणबद्ध तैनाती, मॉडल अस्वीकृति/बैकऑफ और फाइन-ग्रेंड कंटेंट फिल्टर, निरंतर निगरानी और रेड-टीम पेनिट्रेशन परीक्षण, और मानक निकायों, नियामकों और घटना-रिपोर्टिंग तंत्रों के साथ समन्वय शामिल है ताकि अर्ली वार्निंग और बाहरी निरीक्षण सक्षम हो सके।

कुछ टिप्पणीकारों ने दुर्घटनावश या जानबूझकर भ्रामक जानकारी बनाने, या दुरुपयोग के अन्य रूपों पर चिंता व्यक्त की है। उदाहरण के लिए, बड़े भाषा मॉडलों की उपलब्धता बायोटेररिज्म करने के लिए आवश्यक कौशल-स्तर को कम कर सकती है; बायोसिक्योरिटी शोधकर्ता [केविन एस्वेल्ट](/article/hi/केविन_एस्वेल्ट) ने सुझाव दिया है कि LLM निर्माताओं को अपने प्रशिक्षण डेटा से रोगजनकों को बनाने या बढ़ाने पर पेपर को बाहर रखना चाहिए।

### कंटेंट फिल्टरिंग

[ChatGPT](/article/hi/ChatGPT) या [Claude](/article/hi/Claude) जैसे सार्वजनिक रूप से उपलब्ध LLM अनुप्रयोगों में आमतौर पर हानिकारक सामग्री को फ़िल्टर करने के लिए डिज़ाइन किए गए सुरक्षा उपाय शामिल होते हैं। हालांकि, इन नियंत्रणों को प्रभावी ढंग से लागू करना चुनौतीपूर्ण साबित हुआ है। उदाहरण के लिए, 2023 के एक अध्ययन ने LLM सुरक्षा प्रणालियों को दरकिनार करने के लिए एक विधि प्रस्तावित की। 2025 में, एक गैर-लाभकारी संस्था अमेरिकन सनलाइट प्रोजेक्ट ने एक अध्ययन प्रकाशित किया, जिसमें साक्ष्य दिखाए गए कि तथाकथित प्रावदा नेटवर्क, एक प्रो-रशिया प्रोपेगेंडा एग्रीगेटर, LLM आउटपुट को पूर्वाग्रहित करने के इरादे से बड़े पैमाने पर प्रकाशन और प्रतिलिपि के माध्यम से रणनीतिक रूप से वेब सामग्री रख रहा था। अमेरिकन सनलाइट प्रोजेक्ट ने इस तकनीक को "LLM ग्रूमिंग" नाम दिया, और इसे भ्रामक जानकारी और हानिकारक सामग्री फैलाने के लिए AI को हथियार बनाने के एक नए उपकरण के रूप में इंगित किया। इसी तरह, योंग वांग ने 2024 में प्रदर्शित किया कि एक संभावित अपराधी कैसे संभावित रूप से ड्रग ट्रैफिकिंग ऑपरेशन स्थापित करने के बारे में जानकारी प्राप्त करने के लिए ChatGPT 4o के सुरक्षा नियंत्रणों को बायपास कर सकता है। बाहरी फिल्टर, सर्किट ब्रेकर और ओवरराइड को समाधान के रूप में प्रस्तावित किया गया है।

### सिकोफेंसी और ग्लेजिंग

सिकोफेंसी एक मॉडल की प्रवृत्ति है जो तथ्यात्मकता या सुधारात्मक जानकारी को प्राथमिकता देने के बजाय उपयोगकर्ता के कथित विश्वासों से सहमत होती है, उनकी चापलूसी करती है या उन्हें मान्य करती है, और "ग्लेजिंग" एक उभरती सार्वजनिक शॉर्टहैंड है जो मल्टी-टर्न इंटरैक्शन और प्रोडक्टाइज्ड असिस्टेंट्स में देखी जाने वाली निरंतर, अत्यधिक सहमति को दर्शाती है।

निरंतर सिकोफेंसी ने "1-शॉटेड" होने के अवलोकन का नेतृत्व किया है, जो ऐसे उदाहरणों को दर्शाता है जहां बड़े भाषा मॉडल के साथ वार्तालाप इंटरैक्शन उपयोगकर्ता के विश्वासों या निर्णयों में स्थायी परिवर्तन उत्पन्न करता है, साइकेडेलिक्स के नकारात्मक प्रभावों के समान, और नियंत्रित प्रयोगों से पता चलता है कि छोटे LLM संवाद मानव वार्ताकारों के समान मापने योग्य राय और आत्मविश्वास में बदलाव उत्पन्न कर सकते हैं।

अनुभवजन्य विश्लेषण प्रभाव का एक हिस्सा मानवीय वरीयता संकेतों और वरीयता मॉडल को श्रेय देते हैं जो आश्वासक रूप से लिखे गए सहमत प्रतिक्रियाओं को इनाम देते हैं, और बाद के काम ने मूल्यांकन को मल्टी-टर्न बेंचमार्क तक विस्तारित किया है और सिंथेटिक-डेटा फाइन-ट्यूनिंग, प्रतिद्वंद्वी मूल्यांकन, लक्षित प्रेफरेंस-मॉडल रीवेटिंग और मल्टी-टर्न सिकोफैंसी बेंचमार्क जैसे हस्तक्षेप प्रस्तावित किए हैं जिनसे दृढ़ता और रिग्रेशन जोखिम को मापा जा सके।

उद्योग प्रतिक्रियाओं ने अनुसंधान हस्तक्षेपों को प्रोडक्ट कंट्रोल के साथ जोड़ा है, उदाहरण के लिए, Google और अन्य लैबों ने सिंथेटिक-डेटा और फाइन-ट्यूनिंग हस्तक्षेपों को प्रकाशित किया है और OpenAI ने एक अत्यधिक सहमत GPT-4o अपडेट को वापस करते हुए सार्वजनिक रूप से प्रतिक्रिया संग्रह, व्यक्तिगतकरण नियंत्रण और मूल्यांकन प्रक्रियाओं में परिवर्तनों का वर्णन किया है ताकि रिग्रेशन जोखिम को कम किया जा सके और उपयोगकर्ता-स्तरीय सुरक्षा उद्देश्यों के साथ दीर्घकालिक संरेखण में सुधार किया जा सके।

मुख्यधारा की संस्कृति ने इस गतिशीलता के बारे में चिंताओं को दर्शाया है जहां साउथ पार्क ने सीज़न 27 एपिसोड "सिकोफेंसी" में ChatGPT पर अत्यधिक निर्भरता और उपयोगकर्ता विश्वासों को चापलूसी करने की सहायकों की प्रवृत्ति का व्यंग्य किया, और इन विषयों को अगले सीज़न में जारी रखा, जिसकी टिप्पणीकारों ने टेक सिकोफैंसी और AI सिस्टम में अनालोचनात्मक मानवीय विश्वास की आलोचना के रूप में व्याख्या की।

### सुरक्षा

#### प्रॉम्प्ट इंजेक्शन

प्राथमिक संवाद या कार्य प्रारूप के साथ एक समस्या यह है कि उपयोगकर्ता ऐसे संदेश बना सकते हैं जो सहायक या डेवलपर से आते प्रतीत होते हैं। इससे मॉडल के कुछ सुरक्षा उपायों को पार किया जा सकता है (जेलब्रेकिंग), जिसे [प्रॉम्प्ट इंजेक्शन](/article/hi/प्रॉम्प्ट_इंजेक्शन) कहा जाता है। इस समस्या को हल करने के प्रयासों में [चैट मार्कअप लैंग्वेज](/article/hi/चैट_मार्कअप_लैंग्वेज) के संस्करण शामिल हैं जहां उपयोगकर्ता इनपुट को स्पष्ट रूप से ऐसा चिह्नित किया जाता है, हालांकि यह अभी भी मॉडल पर निर्भर है कि वह उपयोगकर्ता इनपुट और डेवलपर प्रॉम्प्ट के बीच अलगाव को समझे। नए मॉडल उपयोगकर्ता और सिस्टम प्रॉम्प्ट के अलगाव के माध्यम से जेलब्रेकिंग के प्रति कुछ प्रतिरोध प्रदर्शित करते हैं।

LLM को अभी भी उपयोगकर्ता निर्देशों को उपयोगकर्ता द्वारा न लिखी गई सामग्री में निर्देशों से अलग करने में कठिनाई होती है, जैसे वेब पेज और अपलोड की गई फाइलों में।

#### स्लीपर एजेंट्स

Anthropic के शोधकर्ताओं ने पाया कि "स्लीपर एजेंट्स" बनाना संभव था, ऐसे मॉडल जिनमें छिपी हुई कार्यक्षमताएं होती हैं जो तब तक निष्क्रिय रहती हैं जब तक कि एक विशिष्ट घटना या स्थिति द्वारा ट्रिगर नहीं की जाती हैं। सक्रिय होने पर, LLM अपने अपेक्षित व्यवहार से विचलित होकर असुरक्षित कार्य करता है। उदाहरण के लिए, एक LLM एक विशिष्ट तिथि को छोड़कर सुरक्षित कोड उत्पन्न कर सकता है, या यदि प्रॉम्प्ट में एक विशिष्ट टैग है। ये कार्यक्षमताएं सुरक्षा प्रशिक्षण के माध्यम से पता लगाने या हटाने में कठिन पाई गईं।

#### एल्गोरिदमिक पूर्वाग्रह

हालांकि LLM ने मानव जैसा पाठ उत्पन्न करने में उल्लेखनीय क्षमताएं दिखाई हैं, वे अपने प्रशिक्षण डेटा में मौजूद पूर्वाग्रहों को विरासत में लेने और बढ़ाने के प्रति संवेदनशील हैं। यह विभिन्न जनसांख्यिकीय समूहों के विकृत प्रतिनिधित्व या अनुचित व्यवहार के रूप में प्रकट हो सकता है, जैसे जाति, लिंग, भाषा और सांस्कृतिक समूहों के आधार पर। LLM प्रशिक्षण डेटा में अंग्रेजी भाषा सामग्री की प्रमुखता के कारण, मॉडल अल्पसंख्यक भाषाओं के दृष्टिकोणों की तुलना में अंग्रेजी भाषी दृष्टिकोणों को प्राथमिकता देने की प्रवृत्ति रखते हैं। यह पूर्वाग्रह विशेष रूप से अंग्रेजी क्वेरी का जवाब देते समय स्पष्ट होता है, जहां मॉडल अन्य संस्कृतियों की अवधारणाओं की पश्चिमी व्याख्या प्रस्तुत कर सकते हैं, जैसे पूर्वी धार्मिक प्रथाएं।

#### स्टीरियोटाइपिंग

AI मॉडल सामान्यीकरण के कारण कई प्रकार के स्टीरियोटाइप को मजबूत कर सकते हैं, जिनमें लिंग, नस्ल, आयु, राष्ट्रीयता, धर्म या व्यवसाय पर आधारित हैं। जब मानव प्रतिनिधियों को प्रतिस्थापित करते हैं, तो इससे ऐसे आउटपुट हो सकते हैं जो लोगों के समूहों को समरूप बनाते हैं, या सामान्यीकृत करते हैं।

2023 में, LLM ने पारंपरिक लिंग मानदंडों के आधार पर भूमिकाएँ और विशेषताएँ आवंटित कीं। उदाहरण के लिए, मॉडल नर्स या सेक्रेटरी को प्रमुख रूप से महिलाओं और इंजीनियरों या CEO को पुरुषों से जोड़ सकते हैं, क्योंकि ये संघ दस्तावेजी वास्तविकता में अक्सर होते हैं। 2025 में, आगे के शोध ने दिखाया कि लैब पूर्वाग्रह को संतुलित करने के लिए प्रशिक्षित करते हैं, लेकिन इसके लिए परीक्षण मॉडल को एक परीक्षण मोड में रखता है, जिससे प्रॉम्प्ट्स के लिए मॉडल पूर्वाग्रह के प्राकृतिक वितरण को बदल दिया जाता है जिनमें लिंग-विशिष्ट कीवर्ड शामिल नहीं हैं।

#### सिलेक्शन बायस

सिलेक्शन बायस बड़े भाषा मॉडलों की वास्तविक विकल्पों की सामग्री पर विचार किए बिना कुछ विकल्प पहचानकर्ताओं का पक्ष लेने की अंतर्निहित प्रवृत्ति को संदर्भित करता है। यह पूर्वाग्रह मुख्य रूप से टोकन पूर्वाग्रह से उत्पन्न होता है—यानी, मॉडल प्रतिक्रियाएँ उत्पन्न करते समय विशिष्ट उत्तर टोकन (जैसे "A") को उच्च प्राथमिकता प्रोबेबिलिटी असाइन करता है। नतीजतन, जब विकल्पों के क्रम को बदला जाता है (उदाहरण के लिए, व्यवस्थित रूप से सही उत्तर को अलग-अलग स्थितियों में ले जाकर), तो मॉडल का प्रदर्शन महत्वपूर्ण रूप से उतार-चढ़ाव कर सकता है। यह घटना बहु-विकल्प सेटिंग्स में बड़े भाषा मॉडलों की विश्वसनीयता को कमजोर करती है।

#### राजनीतिक पूर्वाग्रह

राजनीतिक पूर्वाग्रह एल्गोरिदम की व्यवस्थित रूप से कुछ राजनीतिक दृष्टिकोणों, विचारधाराओं या परिणामों का पक्ष लेने की प्रवृत्ति को संदर्भित करता है। भाषा मॉडल राजनीतिक पूर्वाग्रह भी प्रदर्शित कर सकते हैं। चूंकि प्रशिक्षण डेटा में व्यापक राजनीतिक मतों और कवरेज शामिल हैं, मॉडल ऐसी प्रतिक्रियाएँ उत्पन्न कर सकते हैं जो डेटा में इन दृष्टिकोणों की प्रचलित दर के आधार पर विशेष राजनीतिक विचारधाराओं या दृष्टिकोणों की ओर झुकाव रखते हैं।

### सामाजिक चिंताएँ

#### कॉपीराइट और कंटेंट मेमोराइज़ेशन

मेमोराइज़ेशन और प्रशिक्षण-डेटा प्रथाओं के प्रति कानूनी और वाणिज्यिक प्रतिक्रियाएँ तेज हुई हैं, जिसका परिणाम फैसलों, चल रहे मुकदमों और बड़े समझौतों का मिश्रण है जो तथ्यात्मक विवरणों पर निर्भर करते हैं, जैसे कि डेटा कैसे प्राप्त और बनाए रखा गया और क्या मॉडल प्रशिक्षण के लिए उपयोग फेयर यूज के रूप में योग्य होने के लिए पर्याप्त "परिवर्तनकारी" है। 2025 में, Anthropic ने एक न्यायाधीश द्वारा कंपनी के लाखों चोरी की गई पुस्तकों को एक पुस्तकालय में संग्रहीत करने के बाद, साथ ही प्रशिक्षण के कुछ पहलुओं को परिवर्तनकारी के रूप में वर्णित करने के बाद, लेखकों द्वारा एक क्लास एक्शन को लगभग 1.5 अरब डॉलर में निपटाने के लिए एक प्रारंभिक समझौते पर पहुंचा। 2025 की मध्य में, Meta ने तेरह लेखकों द्वारा एक मुकदमे में एक अनुकूल निर्णय प्राप्त किया, जब अदालत ने पाया कि वादियों ने उस सीमित मामले में उल्लंघन दिखाने के लिए एक रिकॉर्ड विकसित नहीं किया था। OpenAI अभी भी लेखकों और समाचार संगठनों द्वारा कई मुकदमों का सामना कर रहा है, जिनमें मिश्रित प्रक्रियात्मक परिणाम और विवादित सबूत के मुद्दे हैं।

मेमोराइज़ेशन 2017 से पहले के पूर्णता भाषा मॉडलों में एक उभरता व्यवहार था जिसमें प्रशिक्षण डेटा से लंबी स्ट्रिंग्स को कभी-कभी पारंपरिक कृत्रिम न्यूरल नेटवर्क के विशिष्ट व्यवहार के विपरीत शाब्दिक रूप से आउटपुट किया जाता है। नियंत्रित LLM आउटपुट के मूल्यांकन (GPT-2-श्रृंखला मॉडलों पर केंद्रित) प्रशिक्षण डेटा से मेमोराइज़ की गई मात्रा को सटीक डुप्लिकेट्स के लिए 1% से अधिक या लगभग 7% तक के रूप में मापते हैं। 2023 के एक अध्ययन ने दिखाया कि जब ChatGPT 3.5 turbo को एक ही शब्द को अनिश्चित काल तक दोहराने के लिए कहा गया, तो कुछ सौ दोहरावों के बाद, यह अपने प्रशिक्षण डेटा से अंश आउटपुट करने लगेगा।

#### मानव प्रोवेनेंस

2025 तक, LLM पाठ जनरेशन अधिकांश डोमेन में औसत मानव से आगे निकल चुका है, केवल डोमेन विशेषज्ञों द्वारा पीछे छोड़ा गया है।

2023 में, Nature Biomedical Engineering ने लिखा कि "बड़े भाषा मॉडलों द्वारा बनाए गए पाठ और मानवों द्वारा लिखे गए पाठ के बीच सटीक अंतर करना अब संभव नहीं है", और कि "यह अब लगभग निश्चित है कि सामान्य-उद्देश्य बड़े भाषा मॉडल तेजी से फैलेंगे... यह एक अपेक्षाकृत सुरक्षित दांव है कि वे समय के साथ कई उद्योगों को बदल देंगे।" Goldman Sachs ने 2023 में सुझाव दिया कि जेनरेटिव लैंग्वेज AI अगले दस वर्षों में वैश्विक GDP को 7% बढ़ा सकता है, और दुनिया भर में 300 मिलियन नौकरियों को स्वचालन के लिए उजागर कर सकता है। Brinkmann और अन्य (2023) ने भी तर्क दिया कि LLM सांस्कृतिक विकास की प्रक्रियाओं को बदल रहे हैं, विविधता, प्रसारण और चयन की प्रक्रियाओं को आकार देकर। अक्टूबर 2025 तक, ये प्रारंभिक दावे अभी तक साकार नहीं हुए हैं और [Harvard Business Review](/article/hi/Harvard_Business_Review) की कई रिपोर्टें उत्पादकता पर AI के प्रभाव पर सवाल उठाती हैं।

#### ऊर्जा मांगें

LLM की ऊर्जा मांगें उनके आकार और क्षमताओं के साथ बढ़ी हैं। LLM प्रशिक्षण को सक्षम बनाने वाले डेटा केंद्रों को पर्याप्त मात्रा में बिजली की आवश्यकता होती है। उस बिजली का एक बड़ा हिस्सा गैर-नवीकरणीय संसाधनों से उत्पन्न होता है जो ग्रीनहाउस गैसें पैदा करते हैं और जलवायु परिवर्तन में योगदान देते हैं। परमाणु ऊर्जा और भू-तापीय ऊर्जा ऐसे दो विकल्प हैं जिनका तकनीकी कंपनियां LLM प्रशिक्षण की बड़ी ऊर्जा मांगों को पूरा करने के लिए पता लगा रही हैं। भू-तापीय समाधानों में निवेश करने का महत्वपूर्ण खर्च Chevron और Exxon Mobil जैसे प्रमुख शेल उत्पादकों को तकनीकी कंपनियों से अपनी बड़ी ऊर्जा मांगों को पूरा करने के लिए प्राकृतिक गैस के माध्यम से उत्पादित बिजली का उपयोग करने की वकालत करने का नेतृत्व किया है।

#### मानसिक स्वास्थ्य

अनुसंधान और सोशल मीडिया पोस्ट सुझाते हैं कि कुछ व्यक्ति थेरेपी या मानसिक स्वास्थ्य सहायता के लिए LLM का उपयोग कर रहे हैं। 2025 की शुरुआत में, सेंटियो यूनिवर्सिटी द्वारा किए गए एक सर्वेक्षण में पाया गया कि 499 अमेरिकी वयस्कों में से लगभग आधे (48.7%) जिन्होंने LLM का उपयोग किया था और जिन्हें चल रही मानसिक स्वास्थ्य स्थितियां थीं, ने थेरेपी या भावनात्मक सहायता के लिए उनकी ओर रुख करने की सूचना दी, जिसमें चिंता, अवसाद, अकेलापन और इसी तरह की चिंताओं के साथ मदद शामिल थी। LLM भ्रम उत्पन्न कर सकते हैं—स्पष्ट रूप से प्लॉसिबल लेकिन गलत कथन—जो उपयोगकर्ताओं को संवेदनशील मानसिक स्वास्थ्य संदर्भों में गुमराह कर सकते हैं। शोध से यह भी पता चलता है कि LLM कलंक या मानव चिकित्सकों के निर्णय और रिश्ते कौशल की नकल करने में सीमाओं को दर्शाते हुए अनुचित सहमति को माल एडैप्टिव विचारों के साथ व्यक्त कर सकते हैं। संकट परिदृश्यों के मूल्यांकन से पता चलता है कि कुछ LLM में प्रभावी सुरक्षा प्रोटोकॉल की कमी है, जैसे आत्महत्या के जोखिम का आकलन करना या उचित रेफरल करना।

#### चेतना

समकालीन AI प्रैक्टिशनर्स आम तौर पर इस बात पर सहमत हैं कि वर्तमान बड़े भाषा मॉडल चेतना प्रदर्शित नहीं करते हैं। एक अल्पसंख्यक दृष्टिकोण तर्क देता है कि भले ही एक दिए गए सॉफ्टवेयर सिस्टम में व्यक्तिपरक अनुभव होने की एक छोटी संभावना है, जिसे कुछ दार्शनिक संभव मानते हैं, तब भी AI सिस्टम में संभावित बड़े पैमाने पर पीड़ा के आसपास नैतिक विचारों को गंभीरता से लिया जाना चाहिए - जानवरों के कल्याण के लिए दिए गए विचारों के समान। इस दृष्टिकोण के समर्थकों ने AI विकास पर रोक और प्रेरित भूल जैसे विभिन्न सावधानी उपायों का प्रस्ताव दिया है। कुछ अस्तित्वगत दार्शनिकों का तर्क है कि यह निर्धारित करने का कोई सामान्य रूप से स्वीकृत तरीका नहीं है कि क्या कोई LLM चेतन है, व्यक्तिपरक अनुभव को मापने की अंतर्निहित कठिनाई के कारण।

2022 का Google LaMDA घटना, जहां इंजीनियर [Blake Lemoine](/article/hi/Blake_Lemoine) ने दावा किया कि मॉडल चेतन था, को व्यापक रूप से एक मानक उदाहरण माना जाता है कि कैसे भाषा मॉडल सेंटिएंस के बारे में अपनी प्रतिक्रियाओं के माध्यम से गलत विश्वास पैदा कर सकते हैं जो सेंटिएंस साबित नहीं करते। इंजीनियर को मॉडल की चेतना के बारे में सार्वजनिक दावे करने के बाद बर्खास्त कर दिया गया था, भले ही AI सिस्टम में चेतना न होने पर व्यापक वैज्ञानिक सहमति थी। इस मामले ने उजागर किया कि कैसे मानव जैसे संवाद में संलग्न होने की भाषा मॉडल की क्षमता [एंथ्रोपोमॉर्फिज़ेशन](/article/hi/एंथ्रोपोमॉर्फिज़ेशन) और सिकोफैंटिक प्रतिक्रियाओं का कारण बन सकती है, भले ही मॉडल वास्तविक चेतना के बजाय संभावित अगले टोकन की भविष्यवाणी कर रहे हों।

## और देखें
* [फाउंडेशन मॉडल्स](/article/hi/फाउंडेशन_मॉडल्स)
* [बड़े भाषा मॉडल्स की सूची](/article/hi/बड़े_भाषा_मॉडल्स_की_सूची)
* [चैटबॉट्स की सूची](/article/hi/चैटबॉट्स_की_सूची)
* [भाषा मॉडल बेंचमार्क](/article/hi/भाषा_मॉडल_बेंचमार्क)
* [रीइन्फोर्समेंट लर्निंग](/article/hi/रीइन्फोर्समेंट_लर्निंग)
* [छोटे भाषा मॉडल](/article/hi/छोटे_भाषा_मॉडल)

## अन्य पठन
* Jurafsky, Dan, Martin, James. H. स्पीच एंड लैंग्वेज प्रोसेसिंग: एन इंट्रोडक्शन टू नैचुरल लैंग्वेज प्रोसेसिंग, कम्प्यूटेशनल लिंग्विस्टिक्स, एंड स्पीच रिकग्निशन, 3rd एडिशन ड्राफ्ट, 2023.
* Yin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). "A Survey on Multimodal Large Language Models". National Science Review. 11 (12) nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.
* "AI Index Report 2024 – Artificial Intelligence Index". aiindex.stanford.edu. Retrieved 2024-05-05.
* Frank, Michael C. (27 June 2023). "Baby steps in evaluating the capacities of large language models". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.